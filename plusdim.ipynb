{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import dataset\n",
    "import schedulers\n",
    "\n",
    "assert tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# masked version of accuracy and sce\n",
    "def accuracy(real, pred):\n",
    "    acc = tf.keras.metrics.sparse_categorical_accuracy(real, pred)\n",
    "\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(real, 0)), dtype=acc.dtype)\n",
    "    acc *= mask\n",
    "\n",
    "    return tf.reduce_sum(acc) / tf.reduce_sum(mask)\n",
    "\n",
    "def sparse_categorical_crossentropy(y_true, y_pred, sample_weight=None):\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true, 0)), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask) \n",
    "\n",
    "def get_xy(d):\n",
    "    if d is None:\n",
    "        return None\n",
    "    x = d.normalized\n",
    "    y = {'N': d.niqqud, 'D': d.dagesh, 'S': d.sin }\n",
    "    return (x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "corpus['modern'] = dataset.read_corpora([\n",
    "    'hebrew_diacritized/modern'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['mix'] = dataset.read_corpora([\n",
    "    'hebrew_diacritized_private/poetry',\n",
    "    'hebrew_diacritized_private/rabanit',\n",
    "    'hebrew_diacritized_private/pre_modern'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTERS_SIZE = len(dataset.letters_table)\n",
    "NIQQUD_SIZE = len(dataset.niqqud_table)\n",
    "DAGESH_SIZE = len(dataset.dagesh_table)\n",
    "SIN_SIZE = len(dataset.sin_table)\n",
    "WORDSIZE = 9\n",
    "\n",
    "def build_model(units):\n",
    "    inp = keras.Input(shape=(None, WORDSIZE), batch_size=None)\n",
    "    print(f'{inp.shape=}')\n",
    "    embed = layers.Embedding(LETTERS_SIZE, units // WORDSIZE)(inp)\n",
    "    print(f'{embed.shape=}')\n",
    "    \n",
    "    char_layer = layers.TimeDistributed(layers.GRU(units // WORDSIZE, return_sequences=True))(embed)\n",
    "    print(f'{char_layer.shape=}')\n",
    "    \n",
    "    word_layer = layers.Dense(units)(layers.Reshape((-1, units))(embed))\n",
    "    print(f'{word_layer.shape=}')\n",
    "    word_layer = layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(word_layer)\n",
    "    print(f'{word_layer.shape=}')\n",
    "    word_layer = layers.Reshape((-1, WORDSIZE, units // WORDSIZE))(word_layer)\n",
    "    print(f'{word_layer.shape=}')\n",
    "    \n",
    "    combined = layers.Dense(units)(word_layer + char_layer)\n",
    "\n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(combined)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(combined)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(combined)),\n",
    "    ]\n",
    "    return keras.Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "MAXLEN = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp.shape=TensorShape([None, None, 9])\n",
      "embed.shape=TensorShape([None, None, 9, 30])\n",
      "char_layer.shape=TensorShape([None, None, 9, 30])\n",
      "word_layer.shape=TensorShape([None, None, 270])\n",
      "word_layer.shape=TensorShape([None, None, 270])\n",
      "word_layer.shape=TensorShape([None, None, 9, 30])\n",
      "1/1 [==============================] - 0s 0s/step - loss: 5.2357 - N_loss: 2.7888 - D_loss: 1.0755 - S_loss: 1.3714\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, 9)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 9, 30)  1290        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, None, 270)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 270)    73170       reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 270)    1168560     dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, None, 9, 30)  0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 9, 30)  5580        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2_1 (TensorFlow [(None, None, 9, 30) 0           reshape_3[0][0]                  \n",
      "                                                                 time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, None, 9, 270) 8370        tf_op_layer_AddV2_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, None, 9, 16)  4336        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, None, 9, 3)   813         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, None, 9, 4)   1084        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "N (Softmax)                     (None, None, 9, 16)  0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "D (Softmax)                     (None, None, 9, 3)   0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "S (Softmax)                     (None, None, 9, 4)   0           dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,263,203\n",
      "Trainable params: 1,263,203\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "N = 2\n",
    "model = build_model(270)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "x = np.zeros((N, MAXLEN, WORDSIZE))\n",
    "y = [\n",
    "    np.zeros((N, MAXLEN, WORDSIZE)),\n",
    "    np.zeros((N, MAXLEN, WORDSIZE)),\n",
    "    np.zeros((N, MAXLEN, WORDSIZE))\n",
    "]\n",
    "model.evaluate(x, y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 12\n",
    "np.random.seed(2)\n",
    "\n",
    "data = {}\n",
    "data['mix'] = dataset.load_data(corpus['mix'], validation_rate=0.1, maxlen=MAXLEN, wordsize=WORDSIZE)\n",
    "data['modern'] = dataset.load_data(corpus['modern'], validation_rate=0.2, maxlen=MAXLEN, wordsize=WORDSIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_MODE=run\n",
      "inp.shape=TensorShape([None, None, 9])\n",
      "embed.shape=TensorShape([None, None, 9, 66])\n",
      "char_layer.shape=TensorShape([None, None, 9, 66])\n",
      "word_layer.shape=TensorShape([None, None, 594])\n",
      "word_layer.shape=TensorShape([None, None, 594])\n",
      "word_layer.shape=TensorShape([None, None, 9, 66])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/elazarg/dotter\" target=\"_blank\">https://app.wandb.ai/elazarg/dotter</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/elazarg/dotter/runs/3511n5gb\" target=\"_blank\">https://app.wandb.ai/elazarg/dotter/runs/3511n5gb</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to connect to W&B servers after 10 seconds.                    Letting user process proceed while attempting to reconnect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "534/534 [==============================] - 36s 67ms/step - loss: 0.9617 - N_loss: 0.5912 - D_loss: 0.2057 - S_loss: 0.1648 - N_accuracy: 0.8032 - D_accuracy: 0.9191 - S_accuracy: 0.9520 - val_loss: 0.9073 - val_N_loss: 0.5081 - val_D_loss: 0.1938 - val_S_loss: 0.2054 - val_N_accuracy: 0.8394 - val_D_accuracy: 0.9276 - val_S_accuracy: 0.9470\n",
      "Epoch 2/2\n",
      "534/534 [==============================] - 35s 65ms/step - loss: 0.5247 - N_loss: 0.3150 - D_loss: 0.1190 - S_loss: 0.0907 - N_accuracy: 0.8990 - D_accuracy: 0.9566 - S_accuracy: 0.9764 - val_loss: 0.8230 - val_N_loss: 0.4613 - val_D_loss: 0.1807 - val_S_loss: 0.1810 - val_N_accuracy: 0.8561 - val_D_accuracy: 0.9349 - val_S_accuracy: 0.9529\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_MODE run\n",
    "\n",
    "def experiment(lr):\n",
    "    BATCH_SIZE = 128\n",
    "    UNITS = 297 * 2\n",
    "    np.random.seed(2)\n",
    "    model = build_model(units=UNITS)\n",
    "    model.compile(loss=sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                  metrics=accuracy)\n",
    "\n",
    "    model.save_weights('./checkpoints/uninit')\n",
    "    \n",
    "    config = {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'maxlen': MAXLEN,\n",
    "        'wordsize': WORDSIZE,\n",
    "        'units': UNITS,\n",
    "        'model': model,\n",
    "        'lr': lr,\n",
    "        'order': [\n",
    "             ('mix',    (1e-3, 4e-3, 3e-3), 'mix'),\n",
    "#            ('modern', (1e-3, 1e-3, 1e-5), 'modern'),\n",
    "#             ('modern', (40e-4, 40e-4, 1e-5), 'modern_over1'),\n",
    "#             ('modern', (40e-4, 40e-4, 1e-5), 'modern_over2'),\n",
    "        ],\n",
    "    }\n",
    "#     clr = ','.join(str(x) for x in config[\"order\"][0][1])\n",
    "    run = wandb.init(project=\"dotter\",\n",
    "                     group=\"brand_arch\",\n",
    "                     name=f'reshapes_{UNITS}_{WORDSIZE}_{lr}',\n",
    "                     tags=['brand_arch', 'ordered'],\n",
    "                     config=config)\n",
    "\n",
    "    with run:\n",
    "        for kind, clr, save in config['order']:\n",
    "            train, validation = data[kind]\n",
    "\n",
    "            training_data = (x, y) = get_xy(train)\n",
    "            validation_data = get_xy(validation)\n",
    "\n",
    "            wandb_callback = WandbCallback(log_batch_frequency=20,  # int(len(train.normalized) / BATCH_SIZE / 100),\n",
    "                                           training_data=training_data,\n",
    "                                           validation_data=validation_data,\n",
    "                                           log_weights=False)\n",
    "            \n",
    "            scheduler = schedulers.CircularLearningRate(*clr)\n",
    "            scheduler.set_dataset(train, BATCH_SIZE)\n",
    "            callbacks = [wandb_callback]\n",
    "            history = model.fit(x, y, validation_data=validation_data,\n",
    "                                batch_size=BATCH_SIZE, epochs=2, verbose=1, callbacks=callbacks)\n",
    "            \n",
    "            model.save(os.path.join(wandb.run.dir, save + \".h5\"))\n",
    "            model.save_weights('./checkpoints/' + save)\n",
    "    return model\n",
    "\n",
    "for lr in [3e-3]:\n",
    "    model = experiment(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./checkpoints/modern_over')\n",
    "\n",
    "model.compile()\n",
    "model.save('modern.h5')\n",
    "tfjs.converters.save_keras_model(model, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "\n",
    "for n, v in enumerate(['accuracy', 'loss'], 0):\n",
    "    for n1, t in enumerate(['N', 'D', 'S'], 0):\n",
    "        p = ax[n][n1]\n",
    "        p.plot(history.history[t + '_' + v][0:])\n",
    "        p.plot(history.history['val_' + t + '_' +  v][0:])\n",
    "        p.legend([t + '_Train', t + '_Test'], loc='center right')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "test, _ = dataset.load_data(dataset.read_corpora(['test/modernTestCorpus/']), 0, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(units=500, dropout=0.1)\n",
    "model.load_weights('./checkpoints/modern_over2')\n",
    "x = test.normalized\n",
    "y = {'N': test.niqqud, 'D': test.dagesh, 'S': test.sin }\n",
    "\n",
    "model.compile(loss=sparse_categorical_crossentropy,\n",
    "              metrics={'N': accuracy, 'D': accuracy, 'S': accuracy})\n",
    "\n",
    "_ = model.evaluate(x=x, y=y, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "שֶׁיִּקַבֹּל שֶׁיְּקַבֵּל\n",
      "מצוי:  עֶרֶב שַׁבָּת תְּפִלַּת מִנְחָה, כְּדֵי שֶׁיִּקַבֹּל עָלָיו הַשַּׁבָּת מִבְּעוֹד יוֹם. כְּשֶׁיַּגִּיעַ עֵת\n",
      "רצוי:  עֶרֶב שַׁבָּת תְּפִלַּת מִנְחָה, כְּדֵי שֶׁיְּקַבֵּל עָלָיו הַשַּׁבָּת מִבְּעוֹד יוֹם. כְּשֶׁיַּגִּיעַ עֵת\n",
      "97.70% (2 out of 87)\n",
      "91.67% (1 out of 12)\n",
      "\n",
      "בְּגֹּדַר בְּגֶדֶר\n",
      "לִגְנּוֹתָיו, לְגִנּוֹתָיו,\n",
      "וַיִהְיוּ וַיִּהְיוּ\n",
      "הַקּוצִים הַקּוֹצִים\n",
      "לַזָּהָב. לְזָהָב.\n",
      "וְרַד וֶרֶד\n",
      "מצוי:  בְּגֹּדַר -הַקּוֹצִים אֲשֶׁר לִגְנּוֹתָיו, וַיִהְיוּ הַקּוצִים לַזָּהָב. וְכָל וְרַד חַי, אֲשֶׁר נָגְעוּ\n",
      "רצוי:  בְּגֶדֶר -הַקּוֹצִים אֲשֶׁר לְגִנּוֹתָיו, וַיִּהְיוּ הַקּוֹצִים לְזָהָב. וְכָל וֶרֶד חַי, אֲשֶׁר נָגְעוּ\n",
      "87.78% (11 out of 90)\n",
      "50.00% (6 out of 12)\n",
      "\n",
      "-בֶּגֶד -בֶגֶד\n",
      "וּמְכֻסֶּה וּמְכַסָּה\n",
      "מַה מָה\n",
      "מְרֻבֶּה מַרְבֶּה\n",
      "כְּסוּת כְסוּת\n",
      "מצוי:  לֹא -בֶּגֶד וּמְכֻסֶּה – מַה -רַעַשׁ? – אֵין רָע! מְרֻבֶּה כְּסוּת –\n",
      "רצוי:  לֹא -בֶגֶד וּמְכַסָּה – מָה -רַעַשׁ? – אֵין רָע! מַרְבֶּה כְסוּת –\n",
      "85.71% (7 out of 49)\n",
      "44.44% (5 out of 9)\n",
      "\n",
      "שַׁמַּשְׁנִים שֶׁמְּשֻׁנִּים\n",
      "נִרָאוּ נִרְאוּ\n",
      "לְעַיִן. לָעַיִן.\n",
      "בְּנָה בָּנָה\n",
      "בֵּית בַּיִת\n",
      "מֵאַבֵן מֵאֶבֶן\n",
      "אֲדֻמָה אֲדֻמָּה\n",
      "וּמְסְתֶּתַת, וּמְסֻתֶּתֶת,\n",
      "מצוי:  שַׁמַּשְׁנִים נִרָאוּ לְעַיִן. הוּא בְּנָה לוֹ בֵּית מֵאַבֵן אֲדֻמָה וּמְסְתֶּתַת, בַּעַל גַּג\n",
      "רצוי:  שֶׁמְּשֻׁנִּים נִרְאוּ לָעַיִן. הוּא בָּנָה לוֹ בַּיִת מֵאֶבֶן אֲדֻמָּה וּמְסֻתֶּתֶת, בַּעַל גַּג\n",
      "82.28% (14 out of 79)\n",
      "27.27% (8 out of 11)\n",
      "\n",
      "מַלְכוֹת מַלְכוּת\n",
      "נֶאֱדָר נֶאְדָּר\n",
      "גְּדַל גְּדָל\n",
      "-יֶקֶר -יְקָר\n",
      "וְקִדְמוֹן, וְקַדְמוֹן,\n",
      "לִפְנֵי לִפְנַי\n",
      "ולְפְנִים, וְלִפְנִים,\n",
      "בְּחֶבְּיוֹן בְּחֶבְיוֹן\n",
      "עִזוֹ, עֻזּוֹ,\n",
      "מצוי:  מַלְכוֹת נֶאֱדָר אֶחָד, גְּדַל -יֶקֶר וְקִדְמוֹן, כְּאִלּוּ שָׁם לִפְנֵי ולְפְנִים, בְּחֶבְּיוֹן עִזוֹ,\n",
      "רצוי:  מַלְכוּת נֶאְדָּר אֶחָד, גְּדָל -יְקָר וְקַדְמוֹן, כְּאִלּוּ שָׁם לִפְנַי וְלִפְנִים, בְּחֶבְיוֹן עֻזּוֹ,\n",
      "83.53% (14 out of 85)\n",
      "25.00% (9 out of 12)\n",
      "\n",
      "אָרֶךְ אָרֹךְ\n",
      "וְכָהָה וְכֵהֶה\n",
      "פְּתְאמִית פִּתְאֹמִית\n",
      "הָאִיר הֵאִיר\n",
      "יוֹבֵל יוּבַל\n",
      "בִּפְנֵס בְּפַנָּס\n",
      "חָזַק חָזָק\n",
      "שֶׁהִחְזִיק שֶׁהֶחֱזִיק\n",
      "מצוי:  אָרֶךְ וְכָהָה חוֹדֵר דֶּרֶךְ הַחוֹר. בִּמְהִירוּת פְּתְאמִית הָאִיר יוֹבֵל בִּפְנֵס חָזַק שֶׁהִחְזִיק\n",
      "רצוי:  אָרֹךְ וְכֵהֶה חוֹדֵר דֶּרֶךְ הַחוֹר. בִּמְהִירוּת פִּתְאֹמִית הֵאִיר יוּבַל בְּפַנָּס חָזָק שֶׁהֶחֱזִיק\n",
      "82.61% (16 out of 92)\n",
      "33.33% (8 out of 12)\n",
      "\n",
      "לָךָ לָךְ\n",
      "טוֹאָנֶטָה, טוּאַנֶטָה,\n",
      "רוחֲךָ רוּחֵךְ\n",
      "נוֹּחָה נוֹחָה\n",
      "מֵאַהַבָתִי? מֵאַהֲבָתִי?\n",
      "מצוי:  כְּדֵי לְגַלּוֹת לָךָ לִבִּי. הַגִּידִי לִי, טוֹאָנֶטָה, כְּלוּם אֵין רוחֲךָ נוֹּחָה מֵאַהַבָתִי?\n",
      "רצוי:  כְּדֵי לְגַלּוֹת לָךְ לִבִּי. הַגִּידִי לִי, טוּאַנֶטָה, כְּלוּם אֵין רוּחֵךְ נוֹחָה מֵאַהֲבָתִי?\n",
      "89.41% (9 out of 85)\n",
      "58.33% (5 out of 12)\n",
      "\n",
      "מִלְטָרְפָם מִלְּטָרְפָם\n",
      "יוֹבֵל יוּבַל\n",
      "הַסּוֹף הַסּוּף\n",
      "מצוי:  הָיוּ נִמְנָעִים מִלְטָרְפָם לְאוֹר הַיּוֹם. יוֹבֵל הִתְפָּרֵץ לְתוֹךְ הַסּוֹף וְעָשָׂה אֶת דַּרְכּוֹ\n",
      "רצוי:  הָיוּ נִמְנָעִים מִלְּטָרְפָם לְאוֹר הַיּוֹם. יוּבַל הִתְפָּרֵץ לְתוֹךְ הַסּוּף וְעָשָׂה אֶת דַּרְכּוֹ\n",
      "92.86% (6 out of 84)\n",
      "75.00% (3 out of 12)\n",
      "\n",
      "הָעִגּוּרִים הָעֲגוּרִים\n",
      "אֵיבִיקּוס!” אִיבִּיקוֹס!”\n",
      "אֵיבִיקּוֹס?\" אִיבִּיקוֹס?\"\n",
      "מצוי:  – שָׁם -שָׁם – הִנֵּה הָעִגּוּרִים שֶׁל אֵיבִיקּוס!” \" שֶׁל אֵיבִיקּוֹס?\" הַשֵּׁם\n",
      "רצוי:  – שָׁם -שָׁם – הִנֵּה הָעֲגוּרִים שֶׁל אִיבִּיקוֹס!” \" שֶׁל אִיבִּיקוֹס?\" הַשֵּׁם\n",
      "85.71% (9 out of 63)\n",
      "66.67% (3 out of 9)\n",
      "\n",
      "הַמּוֹסוֹת, הַמּוּסוֹת,\n",
      "מֻנַּגֵן מְנַגֵּן\n",
      "בַּלִּיְרָה בַּלִּירָה\n",
      "וְהֶן וְהֵן\n",
      "נִגונוּ, נִגּוּנוֹ,\n",
      "מצוי:  עִם תֵּשַׁע הַמּוֹסוֹת, הוּא מֻנַּגֵן בַּלִּיְרָה וְהֶן מְחוֹלְלוֹת לְפִי נִגונוּ, לְרוֹמֵם אֶת\n",
      "רצוי:  עִם תֵּשַׁע הַמּוּסוֹת, הוּא מְנַגֵּן בַּלִּירָה וְהֵן מְחוֹלְלוֹת לְפִי נִגּוּנוֹ, לְרוֹמֵם אֶת\n",
      "86.90% (11 out of 84)\n",
      "58.33% (5 out of 12)\n",
      "\n",
      "letters: 87.59%, words: 53.10%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/mix')\n",
    "\n",
    "def real_evaluation(data, s=slice(0, None), print_comparison=True):\n",
    "    batch = data.normalized[s]\n",
    "    prediction = model.predict(batch)\n",
    "    [actual_niqqud, actual_dagesh, actual_sin] = [dataset.from_categorical(prediction[0]), dataset.from_categorical(prediction[1]), dataset.from_categorical(prediction[2])]\n",
    "    [expected_niqqud, expected_dagesh, expected_sin] = [data.niqqud[s], data.dagesh[s], data.sin[s]]\n",
    "    actual = dataset.merge(data.text[s], batch, actual_niqqud, actual_dagesh, actual_sin)\n",
    "    expected = dataset.merge(data.text[s], batch, expected_niqqud, expected_dagesh, expected_sin)\n",
    "    total_letters = []\n",
    "    total_words = []\n",
    "    for i, (b, a, e) in enumerate(zip(batch, actual, expected)):\n",
    "        letters = []\n",
    "        letters.extend(expected_niqqud[i][expected_niqqud[i]>0] == actual_niqqud[i][expected_niqqud[i]>0])\n",
    "        letters.extend(expected_dagesh[i][expected_dagesh[i]>0] == actual_dagesh[i][expected_dagesh[i]>0])\n",
    "        letters.extend(expected_sin[i][expected_sin[i]>0] == actual_sin[i][expected_sin[i]>0])\n",
    "        total_letters.extend(letters)\n",
    "        words = []\n",
    "        for aw, ew in zip(a, e):\n",
    "            if len([x for x in 'אבגדהוזחטיכלמנסעפצקרשתךםןףץ' if x in aw]) > 1:\n",
    "                words.append(aw == ew)\n",
    "                if print_comparison and aw != ew:\n",
    "                    print(aw, ew)\n",
    "        total_words.extend(words)\n",
    "        if print_comparison:\n",
    "            print('מצוי: ', ' '.join(a))\n",
    "            print('רצוי: ', ' '.join(e))\n",
    "            print(f'{np.mean(letters):.2%} ({len(letters)-np.sum(letters)} out of {len(letters)})')\n",
    "            print(f'{np.mean(words):.2%} ({len(words)-np.sum(words)} out of {len(words)})')\n",
    "            print()\n",
    "    print(f'letters: {np.mean(total_letters):.2%}, words: {np.mean(total_words):.2%}')\n",
    "\n",
    "real_evaluation(data['mix'][1], s=slice(0, 10), print_comparison=True)  # letters: 95.23%, words: 78.60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import hebrew\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env WANDB_MODE run\n",
    "config = {\n",
    "        'batch_size': 64,\n",
    "        'units': 500,\n",
    "        'order': [\n",
    "            ('mix',    [(30e-4, 80e-4, 1e-4)], 'mix'),\n",
    "            ('modern', [(50e-4, 50e-4, 1e-5)], 'modern'),\n",
    "            ('modern', [(50e-4, 50e-4, 1e-5),\n",
    "                        # (50e-4, 50e-4, 1e-5),\n",
    "                       ], 'modern_over'),\n",
    "        ],\n",
    "    }\n",
    "run = wandb.init(project=\"dotter\",\n",
    "                 # group=\"maxlen\",\n",
    "                 name=f'maxlen_test',\n",
    "                 tags=['CLR', 'ordered'],\n",
    "                 config=config)\n",
    "\n",
    "with run:\n",
    "    for maxlen, letters, words in [\n",
    "            (75, 0.9511, 0.7778),\n",
    "            (80, 0.9531, 0.7819),\n",
    "            (85, 0.9535, 0.7819),\n",
    "            (90, 0.9526, 0.7841),\n",
    "            (95, 0.9514, 0.7795),\n",
    "    ]:\n",
    "        run.log({'maxlen': maxlen,\n",
    "                 'letters': letters,\n",
    "                 'words': words})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['modern'][0].normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_MAXLEN=5\n",
    "EMBED=7\n",
    "UNITS=11\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(keras.Input(shape=(None, WORDSIZE), batch_size=None))\n",
    "print(model.output_shape)\n",
    "model.add(layers.Embedding(LETTERS_SIZE, EMBED))\n",
    "print(model.output_shape)\n",
    "model.add(layers.TimeDistributed(layers.LSTM(UNITS, return_sequences=False)))\n",
    "print(model.output_shape)\n",
    "model.add(layers.LSTM(UNITS, return_sequences=True))\n",
    "print('before', model.output_shape)\n",
    "model.add(layers.Reshape((-1, WORDSIZE, UNITS)))\n",
    "print('after', model.output_shape)\n",
    "# model.add(layers.TimeDistributed(layers.RepeatVector(WORDSIZE)))\n",
    "# print(model.output_shape)\n",
    "model.add(layers.TimeDistributed(layers.LSTM(NIQQUD_SIZE, return_sequences=True)))\n",
    "print(model.output_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit9bb923b013d04c19b7222e7ae44d4e24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
