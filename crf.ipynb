{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import dataset\n",
    "import schedulers\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.config.list_physical_devices('GPU')\n",
    "\n",
    "from tensorflow_addons.layers.crf import CRF\n",
    "from tensorflow_addons.text.crf import crf_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unpack_data(data):\n",
    "    if len(data) == 2:\n",
    "        return data[0], data[1], None\n",
    "    elif len(data) == 3:\n",
    "        return data\n",
    "    else:\n",
    "        raise TypeError(\"Expected data to be a tuple of size 2 or 3.\")\n",
    "\n",
    "\n",
    "accuracy = keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "\n",
    "class ModelWithCRFLoss(tf.keras.Model):\n",
    "    \"\"\"Wrapper around the base model for custom training logic.\"\"\"\n",
    "\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.base_model(inputs)\n",
    "\n",
    "    def compute_loss(self, x, y, sample_weights, training=False):\n",
    "        y_pred = self(x, training=training)\n",
    "        _, potentials, sequence_length, chain_kernel = y_pred\n",
    "        \n",
    "        # potentials: Tensor(\"model_with_crf_loss_4/model_1/N/add_1:0\", shape=(None, 82, 16), dtype=float32)\n",
    "        # expected: A [batch_size, max_seq_len, num_tags] tensor of unary potentials to use as input to the CRF layer.\n",
    "\n",
    "        # sequence_length: Tensor(\"model_with_crf_loss_4/model_1/N/Cast_8:0\", shape=(None,), dtype=int64)\n",
    "        # expected: A [batch_size] vector of true sequence lengths.\n",
    "\n",
    "        # y: {'N': <tf.Tensor 'IteratorGetNext:2' shape=(None, 82) dtype=int32>,\n",
    "        #     'D': <tf.Tensor 'IteratorGetNext:1' shape=(None, 82) dtype=int32>,\n",
    "        #     'S': <tf.Tensor 'IteratorGetNext:3' shape=(None, 82) dtype=int32>}\n",
    "        # expected: A [batch_size, max_seq_len] matrix of tag indices for which we compute the log-likelihood.\n",
    "\n",
    "        # chain_kernel: <tf.Variable 'chain_kernel:0' shape=(16, 16) dtype=float32>\n",
    "        # expected: A [num_tags, num_tags] transition matrix, if available.\n",
    "\n",
    "        crf_loss = -crf_log_likelihood(potentials, y, sequence_length, chain_kernel)[0]\n",
    "\n",
    "        if sample_weights is not None:\n",
    "            crf_loss = crf_loss * sample_weights\n",
    "\n",
    "        return tf.reduce_mean(crf_loss), sum(self.losses)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, y, sample_weight = unpack_data(data)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            crf_loss, internal_losses = self.compute_loss(\n",
    "                x, y, sample_weight, training=True\n",
    "            )\n",
    "            total_loss = crf_loss + internal_losses\n",
    "\n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n",
    "        return {\"crf_loss\": crf_loss, \"internal_losses\": internal_losses, **{m.name: m.result() for m in self.metrics}}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, y, sample_weight = unpack_data(data)\n",
    "        crf_loss, internal_losses = self.compute_loss(x, y, sample_weight)\n",
    "        return {\"crf_loss_val\": crf_loss, \"internal_losses_val\": internal_losses, ** {m.name: m.result() for m in self.metrics}}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_with_crf_loss_11\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nmodel_8 (Model)              [(None, None), (None, Non 271152    \n=================================================================\nTotal params: 271,152\nTrainable params: 271,152\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "LETTERS_SIZE = len(dataset.letters_table)\n",
    "NIQQUD_SIZE = len(dataset.niqqud_table)\n",
    "DAGESH_SIZE = len(dataset.dagesh_table)\n",
    "SIN_SIZE = len(dataset.sin_table)\n",
    "\n",
    "def build_model(EMBED_DIM=28, UNITS=128):\n",
    "    inp = keras.Input(batch_shape=(None, None), batch_size=BATCH_SIZE)\n",
    "    layer = layers.Embedding(LETTERS_SIZE, UNITS, mask_zero=True)(inp)\n",
    "    layer = layers.Bidirectional(layers.LSTM(UNITS, return_sequences=True), merge_mode='sum')(layer)\n",
    "    layer = CRF(NIQQUD_SIZE, name='N')(layer)\n",
    "    model = ModelWithCRFLoss(keras.Model(inputs=inp, outputs=layer))\n",
    "    model.build((None, None))\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "model.summary()\n",
    "model.save_weights('./checkpoints/crf_uninit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_validation):\n",
    "    train, valid = train_validation\n",
    "    model.compile()\n",
    "    callbacks = []\n",
    "        \n",
    "    x  = train.normalized\n",
    "    vx = valid.normalized\n",
    "    \n",
    "    y  = train.niqqud # , 'D': train.dagesh, 'S': train.sin }\n",
    "    vy = valid.niqqud # , 'D': valid.dagesh, 'S': valid.sin }\n",
    "    \n",
    "    return model.fit(x, y, validation_data=(vx, vy), batch_size=BATCH_SIZE, epochs=1, metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "def load_data(source, maxlen=82, validation=0.1):\n",
    "    filenames = [os.path.join('texts', f) for f in source]\n",
    "    train, valid = dataset.load_data(filenames, validation, maxlen=maxlen)\n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ynet = load_data(validation=0.1, source=['modern/ynet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "44/44 [==============================] - 12s 274ms/step - crf_loss: 137.3469 - internal_losses: 0.0000e+00 - val_crf_loss_val: 111.0948 - val_internal_losses_val: 0.0000e+00\n{'crf_loss': [106.46131134033203], 'internal_losses': [0], 'val_crf_loss_val': [111.09480285644531], 'val_internal_losses_val': [0]}\n"
    }
   ],
   "source": [
    " history = fit(data_ynet)\n",
    " print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}