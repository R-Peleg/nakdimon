{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import numpy as np\n",
    "import dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "MAXLEN = 80\n",
    "BATCH_SIZE = 16  # 512\n",
    "data = dataset.load_file(BATCH_SIZE, 0.1, maxlen=MAXLEN,\n",
    "                         filenames=['texts/short_table.txt', 'texts/treasure_island.txt'])  # 'texts/bible.txt', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(16, 80)]                0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (16, 80, 1024)            79872     \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (16, 80, 128)             886272    \n",
      "_________________________________________________________________\n",
      "bidirectional_16 (Bidirectio (16, 80, 128)             198144    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (16, 80, 128)             16512     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (16, 80, 49)              6321      \n",
      "=================================================================\n",
      "Total params: 1,187,121\n",
      "Trainable params: 1,187,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "l2 = tf.keras.regularizers.l2\n",
    "l1 = tf.keras.regularizers.l1\n",
    "\n",
    "EMBED_DIM = 2**10  # larger -> quicker opening. knee at 1024\n",
    "\n",
    "inp = tf.keras.Input(shape=(data.input_texts.shape[1],), batch_size=BATCH_SIZE)\n",
    "\n",
    "h = layers.Embedding(len(data.letters_table), EMBED_DIM, mask_zero=True)(inp)\n",
    "# h = layers.Dropout(DROP)(h)\n",
    "\n",
    "h = layers.Bidirectional(layers.GRU(128, return_sequences=True, recurrent_regularizer=None), merge_mode='sum')(h)  # =l2(1e-6)\n",
    "# h = layers.Dropout(0.5)(h) \n",
    "h1 = layers.Bidirectional(layers.GRU(128, return_sequences=True, recurrent_regularizer=None), merge_mode='sum')(h)\n",
    "# h1 = layers.Dropout(0.5)(h1) \n",
    "h = h1  # layers.Add()([h1, h])\n",
    "    \n",
    "h = layers.Dense(128, activation='relu', kernel_regularizer=l2(5e-5))(h)\n",
    "# h = layers.Dropout(0.5)(h1) \n",
    "#h = layers.Add()([h1, h])\n",
    "# for k in range(1):\n",
    "#     h = layers.Add()([h, Dense(256)(h)])\n",
    "\n",
    "h = layers.Dense(data.niqqud_texts.shape[2])(h)\n",
    "model_niqqud = tf.keras.Model(inputs=[inp], outputs=[h])\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "model_niqqud.compile(loss='mean_squared_logarithmic_error', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "plot_model(model_niqqud, to_file='model.png')\n",
    "model_niqqud.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11880 samples, validate on 1320 samples\n",
      "Epoch 1/10\n",
      "11880/11880 [==============================] - 51s 4ms/sample - loss: 0.0034 - accuracy: 0.8018 - val_loss: 0.0020 - val_accuracy: 0.8733\n",
      "Epoch 2/10\n",
      "11880/11880 [==============================] - 35s 3ms/sample - loss: 0.0016 - accuracy: 0.8966 - val_loss: 0.0014 - val_accuracy: 0.9118\n",
      "Epoch 3/10\n",
      "11880/11880 [==============================] - 35s 3ms/sample - loss: 0.0012 - accuracy: 0.9216 - val_loss: 0.0011 - val_accuracy: 0.9272\n",
      "Epoch 4/10\n",
      "  336/11880 [..............................] - ETA: 32s - loss: 0.0010 - accuracy: 0.9361"
     ]
    }
   ],
   "source": [
    "\n",
    "log_dir = \"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "def fit(EPOCHS):\n",
    "    return model_niqqud.fit(data.input_texts, data.niqqud_texts,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=(data.input_validation, data.niqqud_validation),\n",
    "          callbacks=[\n",
    "              # tf.keras.callbacks.ModelCheckpoint(filepath='niqqud_checkpoints/ckpt_{epoch}', save_weights_only=True),\n",
    "              tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=2, verbose=1),\n",
    "              tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=0, min_lr=0.0001),\n",
    "              # tensorboard_callback,\n",
    "          ]\n",
    "    )\n",
    "\n",
    "history = fit(EPOCHS=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = fit(EPOCHS=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for n, v in enumerate(['accuracy', 'loss'], 1):\n",
    "    plt.subplot(1, 2, n)\n",
    "    plt.plot(history.history[v][0:])\n",
    "    plt.plot(history.history['val_' +  v][0:])\n",
    "    plt.title('Model ' + v)\n",
    "    plt.ylabel(v)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.Model(inputs=[inp], outputs=[tf.keras.layers.Softmax()(h)])\n",
    "\n",
    "def print_predictions(batch):\n",
    "    results = data.merge(batch, ns=model.predict(batch))\n",
    "\n",
    "    for r in results:\n",
    "        print(r)\n",
    "\n",
    "\n",
    "\n",
    "print_predictions(data.input_validation[4:4+BATCH_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(tf.train.latest_checkpoint('niqqud_checkpoints/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs\\fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
