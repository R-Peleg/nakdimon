{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 60\n",
    "BATCH_SIZE = 64\n",
    "files = ['texts/' + f for f in os.listdir('texts/')]\n",
    "\n",
    "data = dataset.load_file(BATCH_SIZE, 0.05, maxlen=MAXLEN, filenames=files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 60, 512)      38400       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Common_RNN (Bidirectional)      (None, 60, 512)      3151872     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Niqqud_RNN (Bidirectional)      (None, 60, 512)      3151872     Common_RNN[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 60, 512)      262656      Niqqud_RNN[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 60, 512)      0           Common_RNN[0][0]                 \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 60, 512)      2048        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "Dagesh_RNN (Bidirectional)      (None, 60, 512)      3151872     Common_RNN[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "N_raw (Dense)                   (None, 60, 60)       30780       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "D_raw (Dense)                   (None, 60, 60)       30780       Dagesh_RNN[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "N (Softmax)                     (None, 60, 60)       0           N_raw[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "D (Softmax)                     (None, 60, 60)       0           D_raw[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 9,820,280\n",
      "Trainable params: 9,819,256\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "l2 = tf.keras.regularizers.l2\n",
    "\n",
    "EMBED_DIM = 512\n",
    "UNITS = 512\n",
    "\n",
    "inp = tf.keras.Input(batch_shape=(None, data.input_texts.shape[1]), batch_size=BATCH_SIZE)\n",
    "\n",
    "embedding = layers.Embedding(len(data.letters_table), EMBED_DIM, mask_zero=True)(inp)\n",
    "\n",
    "rnn_common = layers.Bidirectional(layers.GRU(UNITS, return_sequences=True, dropout=0.5), merge_mode='sum', name='Common_RNN')(embedding)\n",
    "\n",
    "rnn_niqqud = layers.Bidirectional(layers.GRU(UNITS, return_sequences=True, dropout=0.1), merge_mode='sum', name='Niqqud_RNN')(rnn_common)\n",
    "\n",
    "dense_1 = layers.Dense(UNITS, activation='relu', kernel_regularizer=l2(5e-5), kernel_initializer='he_uniform')(rnn_niqqud)\n",
    "\n",
    "add = layers.add([rnn_common, dense_1])\n",
    "\n",
    "norm_1 = layers.BatchNormalization()(add)\n",
    "\n",
    "output_niqqud = layers.Dense(data.niqqud_texts.shape[1], name='N_raw')(norm_1)\n",
    "output_niqqud = layers.Softmax(name='N')(output_niqqud)\n",
    "\n",
    "rnn_dagesh = layers.Bidirectional(layers.GRU(UNITS, return_sequences=True, dropout=0.2), merge_mode='sum', name='Dagesh_RNN')(rnn_common) \n",
    "\n",
    "output_dagesh = layers.Dense(data.dagesh_texts.shape[1], name='D_raw', kernel_initializer='he_uniform')(rnn_dagesh)\n",
    "output_dagesh = layers.Softmax(name='D')(output_dagesh)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inp], outputs=[output_niqqud, output_dagesh])\n",
    "\n",
    "plot_model(model, to_file='model.png')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule(epoch, lr):\n",
    "    # (batch: 64)\n",
    "    # 0.002: 0.9564 0.9864\n",
    "    # 0.0003: 0.9697 0.9897\n",
    "    return [0.002, 0.0003][epoch]\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "def fit(EPOCHS):\n",
    "    return model.fit(data.input_texts, [data.niqqud_texts, data.dagesh_texts],\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=(data.input_validation, [data.niqqud_validation,  data.dagesh_validation]),\n",
    "          callbacks=[\n",
    "              tf.keras.callbacks.LearningRateScheduler(schedule, verbose=0),\n",
    "              # tf.keras.callbacks.ModelCheckpoint(filepath='checkpoints/ckpt_{epoch}', save_weights_only=True),\n",
    "          ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 115337 samples, validate on 6071 samples\n",
      "Epoch 1/2\n",
      "115337/115337 [==============================] - 242s 2ms/sample - loss: 0.3387 - N_loss: 0.2387 - D_loss: 0.0738 - N_accuracy: 0.9187 - D_accuracy: 0.9726 - val_loss: 0.1803 - val_N_loss: 0.1288 - val_D_loss: 0.0342 - val_N_accuracy: 0.9573 - val_D_accuracy: 0.9870\n",
      "Epoch 2/2\n",
      "115200/115337 [============================>.] - ETA: 0s - loss: 0.1366 - N_loss: 0.0936 - D_loss: 0.0292 - N_accuracy: 0.9682 - D_accuracy: 0.9890"
     ]
    }
   ],
   "source": [
    "history = fit(EPOCHS=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "for n, v in enumerate(['accuracy', 'loss'], 0):\n",
    "    for n1, t in enumerate(['D', 'N'], 0):\n",
    "        p = ax[n][n1]\n",
    "        p.plot(history.history[t + '_' + v][0:])\n",
    "        p.plot(history.history['val_' + t + '_' +  v][0:])\n",
    "        p.legend([t + '_Train', t + '_Test'], loc='center right')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(k):\n",
    "    s = slice(k*BATCH_SIZE, (k+1)*BATCH_SIZE)\n",
    "    batch = data.input_validation[s]\n",
    "    [actual_niqqud, actual_dagesh] = dataset.from_categorical(model.predict(batch))\n",
    "    [expected_niqqud, expected_dagesh] = [data.niqqud_validation[s], data.dagesh_validation[s]]\n",
    "    actual = data.merge(batch, ns=actual_niqqud, ds=actual_dagesh)\n",
    "    expected = data.merge(batch, ns=expected_niqqud, ds=expected_dagesh)\n",
    "    for a, e in zip(actual, expected):\n",
    "        print(a)\n",
    "        print(e)\n",
    "        print()\n",
    "\n",
    "print_predictions(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
