{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import dataset\n",
    "assert tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 60\n",
    "BATCH_SIZE = 32\n",
    "files = ['texts/' + f for f in os.listdir('texts/') if not f.startswith('.')]\n",
    "\n",
    "data = dataset.load_file(BATCH_SIZE, 0.05, maxlen=MAXLEN, filenames=files)\n",
    "data.print_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 512\n",
    "UNITS = 256\n",
    "\n",
    "common_input = tf.keras.Input(batch_shape=(None, data.input_texts.shape[1]), batch_size=BATCH_SIZE)\n",
    "common = layers.Embedding(len(data.letters_table), EMBED_DIM, mask_zero=True)(common_input)\n",
    "common = layers.Bidirectional(layers.LSTM(UNITS, return_sequences=True, dropout=0.1), merge_mode='sum')(common)\n",
    "\n",
    "common = layers.add([common, layers.Bidirectional(layers.LSTM(UNITS, return_sequences=True, dropout=0.1), merge_mode='sum')(common)])\n",
    "\n",
    "niqqud = layers.Softmax(name='N')(layers.Dense(len(data.niqqud_table))(common))\n",
    "dagesh = layers.Softmax(name='D')(layers.Dense(len(data.dagesh_table))(common))\n",
    "\n",
    "model = tf.keras.Model(inputs=[common_input], outputs=[niqqud, dagesh])\n",
    "\n",
    "tf.keras.utils.plot_model(model, to_file='model.png')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "def fit(learning_rates):\n",
    "    return model.fit(data.input_texts, [data.niqqud_texts, data.dagesh_texts],\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=len(learning_rates),\n",
    "          validation_data=(data.input_validation, [data.niqqud_validation,  data.dagesh_validation]),\n",
    "          callbacks=[\n",
    "              tf.keras.callbacks.LearningRateScheduler(lambda epoch, lr: learning_rates[epoch], verbose=0),\n",
    "              # tf.keras.callbacks.ModelCheckpoint(filepath='checkpoints/ckpt_{epoch}', save_weights_only=True),\n",
    "          ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = fit([2e-3, 2e-4]) #, 1e-4, 3e-6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "for n, v in enumerate(['accuracy', 'loss'], 0):\n",
    "    for n1, t in enumerate(['D', 'N'], 0):\n",
    "        p = ax[n][n1]\n",
    "        p.plot(history.history[t + '_' + v][0:])\n",
    "        p.plot(history.history['val_' + t + '_' +  v][0:])\n",
    "        p.legend([t + '_Train', t + '_Test'], loc='center right')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "tfjs.converters.save_keras_model(model, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(k):\n",
    "    s = slice(k*BATCH_SIZE, (k+1)*BATCH_SIZE)\n",
    "    batch = data.input_validation[s]\n",
    "    prediction = model.predict(batch)\n",
    "    print([p.shape for p in prediction])\n",
    "    [actual_niqqud, actual_dagesh] = dataset.from_categorical(prediction)\n",
    "    [expected_niqqud, expected_dagesh] = [data.niqqud_validation[s], data.dagesh_validation[s]]\n",
    "    actual = data.merge(batch, ns=actual_niqqud, ds=actual_dagesh)\n",
    "    expected = data.merge(batch, ns=expected_niqqud, ds=expected_dagesh)\n",
    "    for i, (a, e) in enumerate(zip(actual, expected)):\n",
    "        print('מצוי: ', a)\n",
    "        print('רצוי: ', e)\n",
    "        print()\n",
    "\n",
    "print_predictions(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
