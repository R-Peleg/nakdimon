{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import dataset\n",
    "assert tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 200, 512)     22528       input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_18 (Bidirectional (None, 200, 256)     1574912     embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_19 (Bidirectional (None, 200, 256)     1050624     bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 200, 256)     0           bidirectional_18[0][0]           \n",
      "                                                                 bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 200, 16)      4112        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 200, 3)       771         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 200, 4)       1028        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "N (Softmax)                     (None, 200, 16)      0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "D (Softmax)                     (None, 200, 3)       0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "S (Softmax)                     (None, 200, 4)       0           dense_29[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,653,975\n",
      "Trainable params: 2,653,975\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAXLEN = 200\n",
    "\n",
    "def build_model():\n",
    "    EMBED_DIM = 512\n",
    "    UNITS = 256\n",
    "\n",
    "    LETTERS_SIZE = len(dataset.letters_table)\n",
    "    NIQQUD_SIZE = len(dataset.niqqud_table)\n",
    "    DAGESH_SIZE = len(dataset.dagesh_table)\n",
    "    SIN_SIZE = len(dataset.sin_table)\n",
    "\n",
    "    common_input = tf.keras.Input(batch_shape=(None, MAXLEN), batch_size=BATCH_SIZE)\n",
    "    \n",
    "    common = layers.Embedding(LETTERS_SIZE, EMBED_DIM, mask_zero=True)(common_input)\n",
    "    common = layers.Bidirectional(layers.LSTM(UNITS, return_sequences=True, dropout=0.1), merge_mode='sum')(common)\n",
    "    common = layers.add([common,\n",
    "                         layers.Bidirectional(layers.LSTM(UNITS, return_sequences=True, dropout=0.1), merge_mode='sum')(common)])\n",
    "\n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(common)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(common)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(common))\n",
    "    ]\n",
    "    model = tf.keras.Model(inputs=[common_input], outputs=outputs)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    tf.keras.utils.plot_model(model, to_file='model.png')\n",
    "    model.summary()\n",
    "    return model\n",
    "    \n",
    "model = build_model()\n",
    "model.save_weights('./checkpoints/uninit')\n",
    "\n",
    "def fit(data, learning_rates):\n",
    "    x  = data.normalized_texts\n",
    "    vx = data.normalized_validation\n",
    "    y  = {'N': data.niqqud_texts,      'D': data.dagesh_texts,      'S': data.sin_texts,      'C': data.normalized_texts     }\n",
    "    vy = {'N': data.niqqud_validation, 'D': data.dagesh_validation, 'S': data.sin_validation, 'C': data.normalized_validation}\n",
    "    return model.fit(x, y, validation_data=(vx, vy), batch_size=BATCH_SIZE, epochs=len(learning_rates),\n",
    "          callbacks=[\n",
    "              tf.keras.callbacks.LearningRateScheduler(lambda epoch, lr: learning_rates[epoch], verbose=0),\n",
    "              # tf.keras.callbacks.ModelCheckpoint(filepath='checkpoints/ckpt_{epoch}', save_weights_only=True),\n",
    "          ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(source, maxlen=MAXLEN):\n",
    "    filenames = [os.path.join('texts', f) for f in source]\n",
    "    return dataset.load_file(filenames, BATCH_SIZE, 0.1, maxlen=maxlen, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rabanit = load_data(['birkat_hamazon.txt', 'kuzari.txt', 'hakdama_leorot.txt', 'hartzaat_harav.txt', 'orhot_hayim.txt', 'rambam_mamre.txt', 'short_table.txt', 'tomer_dvora.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_modern = load_data(['elef_layla.txt', 'bialik', 'shaul_tchernichovsky', 'breslev.txt', 'itzhak_berkman', 'zevi_scharfstein', 'pesah_kaplan', 'abraham_regelson',\n",
    "                             'elisha_porat', 'uriel_ofek', 'yisrael_dushman', 'zvi_zviri', 'atar_hashabat.txt', 'ali_baba.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modern = load_data(['papers', 'wiki', 'sipurim.txt', 'ricky.txt', 'imagination.txt', 'adamtsair.txt', 'katarsis.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23788 samples, validate on 2644 samples\n",
      "Epoch 1/2\n",
      "23788/23788 [==============================] - 76s 3ms/sample - loss: 0.4776 - N_loss: 0.3718 - D_loss: 0.0897 - S_loss: 0.0160 - N_accuracy: 0.8748 - D_accuracy: 0.9662 - S_accuracy: 0.9958 - val_loss: 0.2132 - val_N_loss: 0.1628 - val_D_loss: 0.0463 - val_S_loss: 0.0040 - val_N_accuracy: 0.9457 - val_D_accuracy: 0.9834 - val_S_accuracy: 0.9990loss: 0.0463 - N_accuracy: 0.7584 - D_accuracy: 0.9379 - S_accuracy:  - ETA: 53s - loss: 0.9211 - N_loss: 0.7141 - D_loss: 0.1613 - S_loss: 0.0 - ETA: 43s - loss: 0.7568 - N_loss: 0.5892 - D_loss: 0.1352 - S_loss: 0.0324 - N_accuracy: 0.8010 - D_accuracy: 0.9482 - S_ - ETA: 42s - loss: 0.7474 - N_loss: 0.5819 - D_loss: 0.1337 - S_loss: 0.0318 - N_accuracy: 0.8034 - D_accuracy: - ETA: 41s - loss: 0.7319 - N_loss: 0.5699 - D_loss: 0.1313 - S_loss: 0.0307 - N_accuracy: 0.8075 - D_accuracy: 0.9497  - ETA: 40s - loss: 0.7214 - N_loss: 0.5619 - D_loss: 0.1296 - S_loss: 0.029 - ETA: 23s - loss: 0.5851 - N_loss: 0.4562 - D_loss:  - ETA: 19s - loss: 0.5607 - N_loss: 0.4372 - D_loss: 0.1034 - S_loss: 0.02 - ETA: 8s - loss: 0.5101 - N_loss: 0.3975 - D_loss:\n",
      "Epoch 2/2\n",
      "23788/23788 [==============================] - 77s 3ms/sample - loss: 0.1898 - N_loss: 0.1450 - D_loss: 0.0413 - S_loss: 0.0036 - N_accuracy: 0.9525 - D_accuracy: 0.9853 - S_accuracy: 0.9992 - val_loss: 0.1727 - val_N_loss: 0.1313 - val_D_loss: 0.0379 - val_S_loss: 0.0034 - val_N_accuracy: 0.9567 - val_D_accuracy: 0.9866 - val_S_accuracy: 0.9992946 - N_loss: 0.1489 - D_loss: 0.0421 - ETA: 27s - loss: 0.1938 - N_loss: 0.1482 - D_loss: 0.0420 - S - ETA: 23s - loss: 0.1932 - N_loss: 0.1 - ETA: 11s - loss: 0.1917 - N_loss: 0.1465 - D_loss: 0.0416 - S_loss: 0.0036 - N_accurac - ETA: 8s - loss: 0.1912 - N_loss: 0.1461 - D_loss: 0.0416 - S_loss: 0.0036 - N_accuracy: 0.9521 - D_accuracy: 0. - ETA: 5s - loss: 0.1906 - N_loss: 0.1456 - D_loss: 0.0415 - S_loss: 0.0036 - N\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/uninit')\n",
    "history = fit(data_rabanit, [3e-3, 3e-4])\n",
    "model.save_weights('./checkpoints/rabanit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 26150 samples, validate on 2906 samples\n",
      "Epoch 1/2\n",
      "26150/26150 [==============================] - 85s 3ms/sample - loss: 0.3017 - N_loss: 0.2294 - D_loss: 0.0657 - S_loss: 0.0066 - N_accuracy: 0.9247 - D_accuracy: 0.9743 - S_accuracy: 0.9982 - val_loss: 0.2408 - val_N_loss: 0.1796 - val_D_loss: 0.0559 - val_S_loss: 0.0053 - val_N_accuracy: 0.9416 - val_D_accuracy: 0.9789 - val_S_accuracy: 0.9986\n",
      "Epoch 2/2\n",
      "26150/26150 [==============================] - 86s 3ms/sample - loss: 0.2062 - N_loss: 0.1520 - D_loss: 0.0493 - S_loss: 0.0049 - N_accuracy: 0.9509 - D_accuracy: 0.9810 - S_accuracy: 0.9987 - val_loss: 0.2055 - val_N_loss: 0.1517 - val_D_loss: 0.0491 - val_S_loss: 0.0048 - val_N_accuracy: 0.9514 - val_D_accuracy: 0.9811 - val_S_accuracy: 0.9987S_loss: 0.0 - ETA: 4s - loss: 0.2065 - N_loss: 0.1522 - D_loss: 0.0494 - S_loss: 0.0049 - N_accuracy:\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/rabanit')\n",
    "history = fit(data_pre_modern, [3e-3, 3e-4])\n",
    "model.save_weights('./checkpoints/pre_modern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1094 samples, validate on 122 samples\n",
      "Epoch 1/2\n",
      "1094/1094 [==============================] - 4s 3ms/sample - loss: 0.3618 - N_loss: 0.2698 - D_loss: 0.0802 - S_loss: 0.0106 - N_accuracy: 0.9234 - D_accuracy: 0.9675 - S_accuracy: 0.9975 - val_loss: 0.2962 - val_N_loss: 0.2177 - val_D_loss: 0.0682 - val_S_loss: 0.0081 - val_N_accuracy: 0.9354 - val_D_accuracy: 0.9721 - val_S_accuracy: 0.9978\n",
      "Epoch 2/2\n",
      "1094/1094 [==============================] - 4s 3ms/sample - loss: 0.2415 - N_loss: 0.1777 - D_loss: 0.0565 - S_loss: 0.0082 - N_accuracy: 0.9489 - D_accuracy: 0.9766 - S_accuracy: 0.9979 - val_loss: 0.2758 - val_N_loss: 0.2032 - val_D_loss: 0.0624 - val_S_loss: 0.0076 - val_N_accuracy: 0.9405 - val_D_accuracy: 0.9745 - val_S_accuracy: 0.9979\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/pre_modern')\n",
    "history = fit(data_modern, [3e-3, 3e-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "for n, v in enumerate(['accuracy', 'loss'], 0):\n",
    "    for n1, t in enumerate(['D', 'N'], 0):\n",
    "        p = ax[n][n1]\n",
    "        p.plot(history.history[t + '_' + v][0:])\n",
    "        p.plot(history.history['val_' + t + '_' +  v][0:])\n",
    "        p.legend([t + '_Train', t + '_Test'], loc='center right')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "tfjs.converters.save_keras_model(model, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "מצוי:  תָּמִיד מִפוּזָרִים פְּקָקִים שֶׁל הַבַּקְבּוּקִים בְּכָל מָקוֹם\", דָּנִית מְגַלָּה. \"אֲנִי חוֹשֶׁבֶת שֶׁכְּשֶׁנִּכְנָסִים לְזוּגִיּוֹת צָרִיךְ לְהִתְרַחֵב וּלְהָבִין שֶׁאֲנַחְנוּ חַיִּים פֹּה בְּיַחַד, וַאֲנַחְנוּ רוֹצִים לַעֲשׂוֹת טוֹב לְקַשֶׁר. חָשׁוּב שֶׁלֹּא יִהְיוּ הִתְחַשְׁבְנוּיוֹת, וּמָה שֶׁצָרִיךְ לַעֲשׂוֹת -\n",
      "רצוי:  תָּמִיד מְפוּזָּרִים פְּקָקִים שֶׁל הַבַּקְבּוּקִים בְּכָל מָקוֹם\", דָּנִית מְגַלָּה. \"אֲנִי חוֹשֶׁבֶת שֶׁכְּשֶׁנִּכְנָסִים לַזּוּגִיּוּת צָרִיךְ לְהִתְרַחֵב וּלְהָבִין שֶׁאֲנַחְנוּ חַיִּים פֹּה בְּיַחַד, וַאֲנַחְנוּ רוֹצִים לַעֲשׂוֹת טוֹב לְקַשֵּׁר. חָשׁוּב שֶׁלֹּא יִהְיוּ הִתְחַשְׁבְּנוּיוֹת, וּמָה שֶׁצָּרִיךְ לַעֲשׂוֹת -\n",
      "\n",
      "מצוי:  טִבְעִיִּים. מַעֲרֶכֶת הַמִּסְפָּרִים הַמִּתְקַבֶּלֶת בְּאוֹפֶן זֶה (מִמֶנָה שֶׁל שְׁנֵי מִסְפָּרִים טִבְעִיִּים) נִקְרֵאת הַיּוֹם הַמִּסְפָּרִים הָרַצְיוֹנָלִיִּים הַחִיּוּבִיִּים. וְאוּלָם הַמָּתֵמָטִיקָאִים הַפִּיתְגוֹרָאִים הֶרָאוּ שֶׁבְּרִיבּוּעַ, שֶׁאוֹרֶךְ צִלְעוּ הוּא 5, לֹא נִיתָּן לְהַצִיג אֶת אוֹרֶךְ הָאֲלַכְסוֹן\n",
      "רצוי:  טִבְעִיִּים. מַעֲרֶכֶת הַמִּסְפָּרִים הַמִּתְקַבֶּלֶת בְּאוֹפֶן זֶה (מִמָּנָה שֶׁל שְׁנֵי מִסְפָּרִים טִבְעִיִּים) נִקְרֵאת הַיּוֹם הַמִּסְפָּרִים הָרַצְיוֹנָלִיִּים הַחִיּוּבִיִּים. וְאוּלָם הַמָּתֵמָטִיקָאִים הַפִּיתָגוֹרָאִים הֶרְאוּ שֶׁבְּרִיבּוּעַ, שֶׁאוֹרֵךְ צַלְעוֹ הוּא 5, לֹא נִיתָּן לְהַצִּיג אֶת אוֹרֶךְ הָאֲלַכְסוֹן\n",
      "\n",
      "מצוי:  שֶׁחָדַר הַשֵּׁינָה הוּא שָׁקֶט וְהוֹלְכִים לִישׁוֹן בּוֹ\". לֶאֱסוֹף חֲפָצִים אוֹ לְהִיפָּטֵר מֵהֶם בְּקַלוּת? הָגָר: \"הָיִיתִי בֶּן אָדָם שֶׁאוֹסֵף, אֲבָל בִּתְקוּפָה הָאַחֲרוֹנָה אֲנִי מְשַׁחְרֶרֶת מָלֵא. נִרְאֶה לִי שֶׁחֵלֵק מֵהַתַהֲלִיךְ בְּתוֹכְנִית הַזֹאת זֶה קְצָת לְשַׁחְרֵר דְּבָרִים מֵהֶעָבָר. עָבַרְתִי\n",
      "רצוי:  שחדר השינה הוא שקט והולכים לישון בו\". לאסוף חפצים או להיפטר מהם בקלות? הגר: \"הייתי בן אדם שאוסף, אבל בתקופה האחרונה אני משחררת מלא. נראה לי שחלק מהתהליך בתוכנית הזאת זה קצת לשחרר דברים מהעבר. עברתי\n",
      "\n",
      "מצוי:  עוֹשִׂים מִכָּל הַלֵּב\". \"טוּשׁ. אֲנִי פָּשׁוּט לֹא אוֹהֶבֶת אַמְבַּטְיָה. לֹא אוֹהֶבֶת לִשְׁהוֹת יוֹתֵר מִדַי זְמַן בַּמַיִם כִּי אָז נִהְיוֹת לִי אֶצְבְּעוֹת סָבְתָא, וְזֶה לֹא כֵּיף אָז מַעֲדִיפָה לַעֲשׂוֹת אֶת זֶה מַהֵר\". טֵלֵוִויזְיָה בַּסָּלוֹן אוֹ בַּחֲדַר הַשֵּׁינָה? \"טֵלֵוִויזְיָה בַּסָּלוֹן.\n",
      "רצוי:  עוֹשִׂים מִכָּל הַלֵּב\". \"טוּשׁ. אֲנִי פָּשׁוּט לֹא אוֹהֶבֶת אַמְבַּטְיָה. לֹא אוֹהֶבֶת לִשְׁהוֹת יוֹתֵר מִדַּי זְמַן בְּמַיִם כִּי אָז נִהְיוֹת לִי אֶצְבְּעוֹת סָבְתָא, וְזֶה לֹא כֵּיף אָז מַעֲדִיפָה לַעֲשׂוֹת אֶת זֶה מַהֵר\". טֵלֵוִויזְיָה בַּסָּלוֹן אוֹ בַּחֲדַר הַשֵּׁינָה? \"טֵלֵוִויזְיָה בַּסָּלוֹן.\n",
      "\n",
      "מצוי:  אָמָּנוּת בֵּינְלְאוּמִּיּוֹת שֶׁיִּשְׂרָאֵל הִיא צָד לָהֶן, אַךְ אֵין לָהֶן תְּחוּלָה מִבְּחִינַת הַמִּשְׁפָּט הַיִּשְׂרְאֵלִי הַפְּנִימִי. דּוּגְמָה לִתְחוּלַת הַמִּשְׁפָּט הַבֵּינְלְאוּמִּי הַמִּנְהָגִי הִיא הַכָּרַת הַמְּדִינָה בַּאֲמְנַת הָאָג מִשְׁנַת 5555 לְעִנְיַין הַלּוֹחֲמָה הַיבַּשְׁתִית הַמּוּכֶּרֶת וְהַנֶאֱכֶפֶת בְּשִׁטְחָה שֶׁל\n",
      "רצוי:  אָמָּנוּת בֵּינְלְאוּמִּיּוֹת שֶׁיִּשְׂרָאֵל הִיא צָד לָהֶן, אַךְ אֵין לָהֶן תְּחוּלָה מִבְּחִינַת הַמִּשְׁפָּט הַיִּשְׂרְאֵלִי הַפְּנִימִי. דּוּגְמָה לִתְחוּלַת הַמִּשְׁפָּט הַבֵּינְלְאוּמִּי הַמִּנְהָגִי הִיא הַכָּרַת הַמְּדִינָה בַּאֲמָנַת הָאג מִשְּׁנַת 5555 לְעִנְיַין הַלּוֹחְמָה הַיַּבַּשְׁתִּית הַמּוּכֶּרֶת וְהַנֶּאֱכֶפֶת בְּשִׁטְחָה שֶׁל\n",
      "\n",
      "מצוי:  בְּנוֹשְׂאֵי תְּזוּנָה וּשְׁאָר אוֹרְחוֹת הַחַיִּים, מִצְטַבְּרוֹת לְגוֹרְמִים בַּעֲלֵי הַשִׁפָּעָה רַבָּה עַל רָמַת הַסִּיכּוּן שֶׁלָּנוּ לְתַחֲלוּאָה. יֵשׁ קֶשֶׁר בָּרוּר וּמוּכָּחַ מַדָעִית בֵּין תְּזוּנָה וְאוֹרְחוֹת חַיִּים לְמַחֲלַת הַסַּרְטָן\", הִיא מַדְגִּישָׁה. \"מֶחְקָרִים מֵהַשָּׁנִים הָאַחֲרוֹנוֹת מַסְבִּירִים אֶת\n",
      "רצוי:  בְּנוֹשְׂאֵי תְּזוּנָה וּשְׁאָר אוֹרְחוֹת הַחַיִּים, מִצְטַבְּרוֹת לְגוֹרְמִים בַּעֲלֵי הַשְׁפָּעָה רַבָּה עַל רָמַת הַסִּיכּוּן שֶׁלָּנוּ לְתַחֲלוּאָה. יֵשׁ קֶשֶׁר בָּרוּר וּמוּכָח מַדָּעִית בֵּין תְּזוּנָה וְאוֹרְחוֹת חַיִּים לְמַחֲלַת הַסַּרְטָן\", הִיא מַדְגִּישָׁה. \"מֶחְקָרִים מֵהַשָּׁנִים הָאַחֲרוֹנוֹת מַסְבִּירִים אֶת\n",
      "\n",
      "מצוי:  מְדִינַת יִשְׂרָאֵל, אַף מִבְּלִי שֶׁתְאוֹמֵץ בִּדְבַר חֲקִיקָה שֶׁל הַכְּנֶסֶת. דּוּגְמָה לִתְחוּלַת הַמִּשְׁפָּט הַבֵּינְלְאוּמִּי הַהֶסְכָּמִי הִיא הַכָּרַת הַמְּדִינָה בָּאֲמָנָה בִּדְבַר הַהַיבָּטִים הָאֶזְרָחִיִּים שֶׁל חֲטִיפָה בֵּינְלְאוּמִית שֶׁל יְלָדִים מִשְׁנַת 5555. אֲמָנָה זוֹ, הָעוֹסֶקֶת בְּמִקְרִים בָּהֶם חָטַף\n",
      "רצוי:  מְדִינַת יִשְׂרָאֵל, אַף מִבְּלִי שֶׁתְּאוּמַּץ בִּדְבַר חֲקִיקָה שֶׁל הַכְּנֶסֶת. דּוּגְמָה לִתְחוּלַת הַמִּשְׁפָּט הַבֵּינְלְאוּמִּי הַהֶסְכֵּמִי הִיא הַכָּרַת הַמְּדִינָה בָּאֲמָנָה בִּדְבַר הַהֶיבֵּטִים הָאֶזְרָחִיִּים שֶׁל חֲטִיפָה בֵּינְלְאוּמִּית שֶׁל יְלָדִים מִשְּׁנַת 5555. אֲמָנָה זוֹ, הָעוֹסֶקֶת בְּמִקְרִים בָּהֶם חָטַף\n",
      "\n",
      "מצוי:  בֵּית הַמִּשְׁפָּט הַנּוֹבַעַת מֵהַחְלָטוֹתָיו מְקַדֶּמַת דָּנָא. אַךְ בְּעוֹד שֶׁהָעוֹנֵשׁ עַל פִּי הַמִּשְׁפָּט הַמְּקוּבָּל עַל עֲבֵירָה זוֹ הוּא עוֹנֶשׁ מָווֹת, הֲרֵי שֶׁחֲקִיקָה שֶׁל הַפְּרְלָמֶנְט קָבְעָה כִּי הָעוֹנֶשׁ עַל עֲבֵירָה זוֹ יִהְיֶה מַאֲסַר עוֹלָם. שִׁיטָה זוֹ נְהוּגָה, בְּשִׁינּוּיִים מְסוּיָּמִים (אִם\n",
      "רצוי:  בֵּית הַמִּשְׁפָּט הַנּוֹבַעַת מֵהַחְלָטוֹתָיו מְקַדֶּמֶת דְּנָא. אַךְ בְּעוֹד שֶׁהָעוֹנֶשׁ עַל פִּי הַמִּשְׁפָּט הַמְּקוּבָּל עַל עֲבֵירָה זוֹ הוּא עוֹנֶשׁ מָוֶות, הֲרֵי שֶׁחֲקִיקָה שֶׁל הַפַּרְלָמֶנְט קָבְעָה כִּי הָעוֹנֶשׁ עַל עֲבֵירָה זוֹ יִהְיֶה מַאֲסַר עוֹלָם. שִׁיטָה זוֹ נְהוּגָה, בְּשִׁינּוּיִים מְסוּיָּמִים (אִם\n",
      "\n",
      "מצוי:  פְּסִיבִי, וְעָלָיו לְהִסְתַּפֵּק בִּרְאָיוֹת אוֹתָן מְבִיאִים הַצְּדָדִים בְּפָנָיו כְּבְסִיס לְהַכְרָעָתוֹ. בְּנִיגּוּד לְכָךְ, בִּמְדִינוֹת הַמִּשְׁפָּט הַקּוֹנְטִינֶנְטְלִי קַיֶּימֶת הַשִּׁיטָה הָאִינְקְוויזִיטוֹרִית, לְפִיהָ לְשׁוֹפֵט (הַקָּרוּי לְעִיתִים \"שׁוֹפֵט חוֹקֵר\") חֵלֶק בִּמְצִיאַת הָרְאָיוֹת, וְתַפְקִיד\n",
      "רצוי:  פָּסִיבִי, וְעָלָיו לְהִסְתַּפֵּק בִּרְאָיוֹת אוֹתָן מְבִיאִים הַצְּדָדִים בְּפָנָיו כְּבָסִיס לְהַכְרָעָתוֹ. בְּנִיגּוּד לְכָךְ, בִּמְדִינוֹת הַמִּשְׁפָּט הַקּוֹנְטִינֶנְטָלִי קַיֶּימֶת הַשִּׁיטָה הָאִינְקְוִויזִיטוֹרִית, לְפִיהָ לַשּׁוֹפֵט (הַקָּרוּי לְעִיתִּים \"שׁוֹפֵט חוֹקֵר\") חֵלֶק בִּמְצִיאַת הָרְאָיוֹת, וְתַפְקִיד\n",
      "\n",
      "מצוי:  וְאָדִיב, בַּעַל גִּינוֹנִים אֵירוֹפִיִּים וּמְמוֹצָא רוּסִי. דֵּנִיס נִדְלֵק עַל הֲדַר מִמַבָּט רִאשׁוֹן, אֲבָל בַּהַתְחָלָה הִיא הִגִיבָה בִּקְרִירוֹת. מֵאָז הַיְּחָסִים הִתְפַּתְּחוּ בְּכָל פֶּרֶק אֶפְשָׁר לִרְאוֹת אֶת הַקְרְבָה וְהַחֲבֵרוֹת שֶׁנּוֹצְרוֹת. הַשְּׁנַיִים עָבְרוּ לָגוּר בַּדִּירָה הַיָּפָה וְהַמְּרווּחַת\n",
      "רצוי:  וְאָדִיב, בַּעַל גִּינּוּנִים אֵירוֹפִּיִּים וּמִמּוֹצָא רוּסִי. דֵּנִיס נִדְלַק עַל הֲדַר מִמַּבָּט רִאשׁוֹן, אֲבָל בַּהַתְחָלָה הִיא הֵגִיבָה בִּקְרִירוּת. מֵאָז הַיְּחָסִים הִתְפַּתְּחוּ בְּכָל פֶּרֶק אֶפְשָׁר לִרְאוֹת אֶת הַקִּרְבָה וְהַחֲבָרוֹת שֶׁנּוֹצָרוֹת. הַשְּׁנַיִים עָבְרוּ לָגוּר בַּדִּירָה הַיָּפָה וְהַמְּרוּוַּחַת\n",
      "\n",
      "מצוי:  בַּרְקַע, זֶה הָיָה מְאוֹד דּוֹמִינֶנְטִי\". הִיא עַצְמָה הֵבִיאָה תְּמוּנוֹת וּתְמוּנוֹת שֶׁל הַמִּשְׁפָּחָה - \"אֲנִי מְאוֹד קְרוֹבָה לְמִשְׁפָּחָה שֶׁלִּי, וְהַתְּמוּנוֹת שֶׁלָּהֶם נוֹתְנוֹת לִי תְּחוּשַׁת שַׁיּיכוֹת\", הִיא אוֹמֶרֶת. אֶת הַקַרֵדִיט לְעִיצּוּב הֶחָלָל כָּל אֶחָד מֵהֶם נוֹתֵן לְאַחֵר. \"ניר\n",
      "רצוי:  בָּרֶקַע, זֶה הָיָה מְאוֹד דּוֹמִינַנְטִי\". הִיא עַצְמָהּ הֵבִיאָה תְּמוּנוֹת וּתְמוּנוֹת שֶׁל הַמִּשְׁפָּחָה - \"אֲנִי מְאוֹד קְרוֹבָה לַמִּשְׁפָּחָה שֶׁלִּי, וְהַתְּמוּנוֹת שֶׁלָּהֶם נוֹתְנוֹת לִי תְּחוּשַׁת שַׁיָּיכוּת\", הִיא אוֹמֶרֶת. אֶת הַקְּרֵדִיט לְעִיצּוּב הֶחָלָל כָּל אֶחָד מֵהֶם נוֹתֵן לָאַחֵר. \"נִיר\n",
      "\n",
      "מצוי:  כְּמוֹ מְשִׂימָה בִּלְתִּי אֶפְשָׁרִית. בְּדֶרֶךְ כְּלָל מִישֶׁהוּ אֶחָד עַל הַבֵּיצִים, מִישֶׁהוּ אַחֵר עַל סָלֶט, וְקְנִיּוֹת עוֹשִׂים בְּיַחַד. בַּדִּירָה יֵשׁ מֵדִיחַ, אֲבָל אֲנַחְנוּ בְּקוֹשִׁי מְנַצְּלִים אוֹתוֹ כִּי אָמִיר שׁוֹטֵף רוֹב הַזְמַן אֶת הַכֵּלִים, הוּא מִצְטַיֵּין בָּזֶה וְגַם בִּכְבִיסוֹת. אֲנִי\n",
      "רצוי:  כְּמוֹ מְשִׂימָה בִּלְתִּי אֶפְשָׁרִית. בְּדֶרֶךְ כְּלָל מִישֶׁהוּ אֶחָד עַל הַבֵּיצִים, מִישֶׁהוּ אַחֵר עַל סָלָט, וּקְנִיּוֹת עוֹשִׂים בְּיַחַד. בַּדִּירָה יֵשׁ מַדִּיחַ, אֲבָל אֲנַחְנוּ בְּקוֹשִׁי מְנַצְּלִים אוֹתוֹ כִּי אָמִיר שׁוֹטֵף רוֹב הַזְּמַן אֶת הַכֵּלִים, הוּא מִצְטַיֵּין בְּזֶה וְגַם בִּכְבִיסוֹת. אֲנִי\n",
      "\n",
      "מצוי:  אֶפְשָׁרוּת לְבַחוֹר לַעֲשׂוֹת מָה שֶׁהוּא רוֹצֶה\". לְדִּירָה הַמְּשׁוּתֶּפֶת שֶׁעוֹצְבָה עַל יְדֵי מְעַצֶּבֶת הַפָּנִים מאי מוּרְגֶנְשְׁטִיְין, הֵבִיאָה רוֹנִי אֶת אַרְבַּע הַמִּזְווֹדוֹת שֶׁהֵבִיאָה מִחוֹ\"ל (\"יֵשׁ שָׁם בְּעֶרֶךְ אֶת כָּל הַחַיִּים שֶׁלִי\"), וְהִיא מִתְגַּעְגַעַת לְכֶּלֶב שֶׁלָהּ, עוֹזִי. \"אֲנִי\n",
      "רצוי:  אפשרות לבחור לעשות מה שהוא רוצה\". לדירה המשותפת שעוצבה על ידי מעצבת הפנים מאי מורגנשטיין, הביאה רוני את ארבע המזוודות שהביאה מחו\"ל (\"יש שם בערך את כל החיים שלי\"), והיא מתגעגעת לכלב שלה, עוזי. \"אני\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_predictions(data, k):\n",
    "    s = slice(k*BATCH_SIZE, (k+1)*BATCH_SIZE)\n",
    "    batch = data.normalized_validation[s]\n",
    "    prediction = model.predict(batch)\n",
    "    [actual_niqqud, actual_dagesh, actual_sin] = [dataset.from_categorical(prediction[0]), dataset.from_categorical(prediction[1]), dataset.from_categorical(prediction[2])]\n",
    "    [expected_niqqud, expected_dagesh, expected_sin] = [data.niqqud_validation[s], data.dagesh_validation[s], data.sin_validation[s]]\n",
    "    actual = data.merge(batch, ns=actual_niqqud, ds=actual_dagesh, ss=actual_sin)\n",
    "    expected = data.merge(batch, ns=expected_niqqud, ds=expected_dagesh, ss=expected_sin)\n",
    "    for i, (a, e) in enumerate(zip(actual, expected)):\n",
    "        print('מצוי: ', a)\n",
    "        print('רצוי: ', e)\n",
    "        print()\n",
    "\n",
    "print_predictions(load_data(['papers', 'wiki'], maxlen=MAXLEN), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
