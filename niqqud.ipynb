{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "import os\n",
    "os.environ['TF_KERAS'] = '1'\n",
    "\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "# from keras_radam import RAdam\n",
    "import numpy as np\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "\n",
    "MAXLEN = 50\n",
    "BATCH_SIZE = 128  # 512\n",
    "files = ['texts/' + f for f in os.listdir('texts/')]  # short_table.txt', 'texts/treasure_island.txt', 'texts/ahava.txt', 'texts/rambam_mamre.txt', 'texts/ali_baba.txt', 'texts/bible.txt']\n",
    "\n",
    "data = dataset.load_file(BATCH_SIZE, 0.07, maxlen=MAXLEN, filenames=files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(128, 50)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (128, 50, 256)       19200       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Common_RNN (Bidirectional)      (128, 50, 512)       2365440     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Niqqud_RNN (Bidirectional)      (128, 50, 512)       3151872     Common_RNN[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (128, 50, 512)       262656      Niqqud_RNN[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (128, 50, 512)       0           Common_RNN[0][0]                 \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (128, 50, 512)       2048        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "Dagesh_RNN (Bidirectional)      (128, 50, 512)       3151872     Common_RNN[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "N_raw (Dense)                   (128, 50, 50)        25650       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "D_raw (Dense)                   (128, 50, 50)        25650       Dagesh_RNN[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "N (Softmax)                     (128, 50, 50)        0           N_raw[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "D (Softmax)                     (128, 50, 50)        0           D_raw[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 9,004,388\n",
      "Trainable params: 9,003,364\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "l2 = tf.keras.regularizers.l2\n",
    "\n",
    "EMBED_DIM = 256  # larger -> quicker opening. knee at 1024\n",
    "UNITS = 512\n",
    "\n",
    "inp = tf.keras.Input(shape=(data.input_texts.shape[1],), batch_size=BATCH_SIZE)\n",
    "\n",
    "embedding = layers.Embedding(len(data.letters_table), EMBED_DIM, mask_zero=True)(inp)\n",
    "\n",
    "rnn_common = layers.Bidirectional(layers.GRU(UNITS, return_sequences=True, dropout=0.5), merge_mode='sum', name='Common_RNN')(embedding)\n",
    "\n",
    "rnn_niqqud = layers.Bidirectional(layers.GRU(UNITS, return_sequences=True, dropout=0.1), merge_mode='sum', name='Niqqud_RNN')(rnn_common)\n",
    "\n",
    "dense_1 = layers.Dense(UNITS, activation='relu', kernel_regularizer=l2(5e-5), kernel_initializer='he_uniform')(rnn_niqqud)\n",
    "\n",
    "add = layers.add([rnn_common, dense_1])\n",
    "\n",
    "norm_1 = layers.BatchNormalization()(add)\n",
    "\n",
    "output_niqqud = layers.Dense(data.niqqud_texts.shape[1], name='N_raw')(norm_1)\n",
    "output_niqqud = layers.Softmax(name='N')(output_niqqud)\n",
    "\n",
    "rnn_dagesh = layers.Bidirectional(layers.GRU(UNITS, return_sequences=True, dropout=0.2), merge_mode='sum', name='Dagesh_RNN')(rnn_common) \n",
    "\n",
    "output_dagesh = layers.Dense(data.dagesh_texts.shape[1], name='D_raw', kernel_initializer='he_uniform')(rnn_dagesh)\n",
    "output_dagesh = layers.Softmax(name='D')(output_dagesh)\n",
    "\n",
    "model = tf.keras.Model(inputs=[inp], outputs=[output_niqqud, output_dagesh])\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# radam = RAdam(min_lr=1e-3)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "plot_model(model, to_file='model.png')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 132249 samples, validate on 14695 samples\n",
      "132249/132249 [==============================] - 204s 2ms/sample - loss: 0.4979 - N_loss: 0.3418 - D_loss: 0.1201 - N_accuracy: 0.8822 - D_accuracy: 0.9563 - val_loss: 0.2567 - val_N_loss: 0.1694 - val_D_loss: 0.0605 - val_N_accuracy: 0.9420 - val_D_accuracy: 0.9764\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# log_dir = \"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "def fit(EPOCHS):\n",
    "    return model.fit(data.input_texts, [data.niqqud_texts, data.dagesh_texts],\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=(data.input_validation, [data.niqqud_validation,  data.dagesh_validation]),\n",
    "          callbacks=[\n",
    "              # tf.keras.callbacks.ModelCheckpoint(filepath='niqqud_checkpoints/ckpt_{epoch}', save_weights_only=True),\n",
    "              # tf.keras.callbacks.EarlyStopping(patience=2, verbose=1), # monitor='accuracy', \n",
    "              # tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=0, min_lr=0.0001), # monitor='loss', \n",
    "              # tensorboard_callback,\n",
    "          ]\n",
    "    )\n",
    "\n",
    "history = fit(EPOCHS=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 132249 samples, validate on 14695 samples\n",
      "132249/132249 [==============================] - 183s 1ms/sample - loss: 0.0796 - N_loss: 0.0550 - D_loss: 0.0141 - N_accuracy: 0.9804 - D_accuracy: 0.9946 - val_loss: 0.1206 - val_N_loss: 0.0849 - val_D_loss: 0.0251 - val_N_accuracy: 0.9736 - val_D_accuracy: 0.9916\n"
     ]
    }
   ],
   "source": [
    "history = fit(EPOCHS=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 132249 samples, validate on 14695 samples\n",
      "130816/132249 [============================>.] - ETA: 1s - loss: 0.0760 - N_loss: 0.0524 - D_loss: 0.0132 - N_accuracy: 0.9812 - D_accuracy: 0.9949"
     ]
    }
   ],
   "source": [
    "history = fit(EPOCHS=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "for n, v in enumerate(['accuracy', 'loss'], 0):\n",
    "    for n1, t in enumerate(['D', 'N'], 0):\n",
    "        p = ax[n][n1]  # plt.subplot(1, 4, i)\n",
    "        p.plot(history.history[t + '_' + v][0:])\n",
    "        p.plot(history.history['val_' + t + '_' +  v][0:])\n",
    "        # p.title('Model ' + t + ' ' + v)\n",
    "        # p.ylabel(v)\n",
    "        # p.xlabel('Epoch')\n",
    "        p.legend([t + '_Train', t + '_Test'], loc='center right')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Model(inputs=[inp], outputs=[tf.keras.layers.Softmax()(output_niqqud), tf.keras.layers.Softmax()(output_dagesh)])\n",
    "\n",
    "def print_predictions(k):\n",
    "    s = slice(k*BATCH_SIZE, (k+1)*BATCH_SIZE)\n",
    "    batch = data.input_validation[s]\n",
    "    [actual_niqqud, actual_dagesh] = dataset.from_categorical(model.predict(batch))\n",
    "    [expected_niqqud, expected_dagesh] = [data.niqqud_validation[s], data.dagesh_validation[s]]\n",
    "    actual = data.merge(batch, ns=actual_niqqud, ds=actual_dagesh)\n",
    "    expected = data.merge(batch, ns=expected_niqqud, ds=expected_dagesh)\n",
    "    for a, e in zip(actual, expected):\n",
    "        print(a)\n",
    "        print(e)\n",
    "        print()\n",
    "\n",
    "print_predictions(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(tf.train.latest_checkpoint('niqqud_checkpoints/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs\\fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
