{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import dataset\n",
    "import utils\n",
    "assert tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 64, 110)      4840        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 64, 220)      582560      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 64, 220)      776160      bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 64, 220)      0           bidirectional[0][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64, 16)       3536        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64, 3)        663         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64, 4)        884         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "N (Softmax)                     (None, 64, 16)       0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "D (Softmax)                     (None, 64, 3)        0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "S (Softmax)                     (None, 64, 4)        0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 1)            888         add[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 1,369,531\n",
      "Trainable params: 1,369,531\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAXLEN = 64\n",
    "\n",
    "LETTERS_SIZE = len(dataset.letters_table)\n",
    "NIQQUD_SIZE = len(dataset.niqqud_table)\n",
    "DAGESH_SIZE = len(dataset.dagesh_table)\n",
    "SIN_SIZE = len(dataset.sin_table)\n",
    "\n",
    "def build_model(EMBED_DIM=110, UNITS=220):\n",
    "\n",
    "    layer = input_text = tf.keras.Input(batch_shape=(None, MAXLEN), batch_size=BATCH_SIZE)\n",
    "    \n",
    "    layer = layers.Embedding(LETTERS_SIZE, EMBED_DIM, mask_zero=True)(layer)\n",
    "    layer = layers.Bidirectional(layers.LSTM(UNITS, return_sequences=True, dropout=0.0), merge_mode='sum')(layer)\n",
    "    layer = layers.add([layer,\n",
    "            layers.Bidirectional(layers.LSTM(UNITS, return_sequences=True, dropout=0.0), merge_mode='sum')(layer)])\n",
    "    \n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(layer)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(layer)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(layer))\n",
    "    ]\n",
    "    model = tf.keras.Model(inputs=[input_text], outputs=outputs)\n",
    "\n",
    "    # tf.keras.utils.plot_model(model, to_file='model.png')\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "model.summary()\n",
    "model.save_weights('./checkpoints/uninit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(data, scheduler, verbose=1):\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    callbacks = []\n",
    "    if isinstance(scheduler, utils.CircularLearningRate):\n",
    "        scheduler.set_dataset(data, BATCH_SIZE)\n",
    "    if scheduler:\n",
    "        callbacks.append(scheduler)\n",
    "    x  = data.normalized_texts\n",
    "    vx = data.normalized_validation\n",
    "    y  = {'N': data.niqqud_texts,      'D': data.dagesh_texts,      'S': data.sin_texts,      'C': data.normalized_texts     }\n",
    "    vy = {'N': data.niqqud_validation, 'D': data.dagesh_validation, 'S': data.sin_validation, 'C': data.normalized_validation}\n",
    "    return model.fit(x, y, validation_data=(vx, vy), batch_size=BATCH_SIZE, epochs=1, verbose=verbose, callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(source, maxlen=MAXLEN, validation=0.1):\n",
    "    filenames = [os.path.join('texts', f) for f in source]\n",
    "    return dataset.load_file(filenames, BATCH_SIZE, validation, maxlen=maxlen, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rabanit = load_data(['birkat_hamazon.txt', 'kuzari.txt', 'hakdama_leorot.txt', 'hartzaat_harav.txt', 'orhot_hayim.txt', 'rambam_mamre.txt', 'short_table.txt', 'tomer_dvora.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_modern = load_data(['elef_layla.txt', 'bialik', 'shaul_tchernichovsky', 'breslev.txt', 'itzhak_berkman', 'zevi_scharfstein', 'pesah_kaplan', 'abraham_regelson',\n",
    "                             'elisha_porat', 'uriel_ofek', 'yisrael_dushman', 'zvi_zviri', 'atar_hashabat.txt', 'ali_baba.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modern = load_data(validation=0.2, source=['forums', 'newspapers', 'wiki', 'blogs', 'adamtsair.txt', 'katarsis.txt'])  # , 'imagination.txt', 'sipurim.txt', 'ricky.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 76377 samples, validate on 8487 samples\n",
      "76377/76377 [==============================] - 92s 1ms/sample - loss: 0.3310 - N_loss: 0.2578 - D_loss: 0.0634 - S_loss: 0.0097 - N_accuracy: 0.9099 - D_accuracy: 0.9759 - S_accuracy: 0.9975 - val_loss: 0.1308 - val_N_loss: 0.0980 - val_D_loss: 0.0302 - val_S_loss: 0.0024 - val_N_accuracy: 0.9672 - val_D_accuracy: 0.9893 - val_S_accuracy: 0.9994\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/uninit')\n",
    "history = fit(data_rabanit, utils.CircularLearningRate(20e-4, 50e-4, 5e-4))\n",
    "model.save_weights('./checkpoints/rabanit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 84096 samples, validate on 9344 samples\n",
      "84096/84096 [==============================] - 92s 1ms/sample - loss: 0.2547 - N_loss: 0.1918 - D_loss: 0.0570 - S_loss: 0.0059 - N_accuracy: 0.9356 - D_accuracy: 0.9771 - S_accuracy: 0.9984 - val_loss: 0.1905 - val_N_loss: 0.1396 - val_D_loss: 0.0461 - val_S_loss: 0.0047 - val_N_accuracy: 0.9537 - val_D_accuracy: 0.9817 - val_S_accuracy: 0.9988ss: 0.2567 - N_loss: 0.1934 - D_loss: 0.0573 - S_loss: 0.0060 - N_accuracy: 0.9350 - D_accuracy:  - ETA: 0s - loss: 0.2553 - N_loss: 0.1923 - D_loss: 0.0571 - S_loss: 0.0059 - N_accuracy: 0.9354 - D_accuracy: 0.9771 - S_acc\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/rabanit')\n",
    "history = fit(data_pre_modern, utils.CircularLearningRate(20e-4, 40e-4, 0.1e-4))\n",
    "model.save_weights('./checkpoints/pre_modern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12236 samples, validate on 3060 samples\n",
      "12236/12236 [==============================] - 24s 2ms/sample - loss: 0.2321 - N_loss: 0.1765 - D_loss: 0.0502 - S_loss: 0.0056 - N_accuracy: 0.9461 - D_accuracy: 0.9812 - S_accuracy: 0.9988 - val_loss: 0.1733 - val_N_loss: 0.1296 - val_D_loss: 0.0390 - val_S_loss: 0.0044 - val_N_accuracy: 0.9614 - val_D_accuracy: 0.9859 - val_S_accuracy: 0.9990\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/pre_modern')\n",
    "history = fit(data_modern, utils.CircularLearningRate(6e-3, 6e-3, 0.5e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "for n, v in enumerate(['accuracy', 'loss'], 0):\n",
    "    for n1, t in enumerate(['D', 'N'], 0):\n",
    "        p = ax[n][n1]\n",
    "        p.plot(history.history[t + '_' + v][0:])\n",
    "        p.plot(history.history['val_' + t + '_' +  v][0:])\n",
    "        p.legend([t + '_Train', t + '_Test'], loc='center right')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "tfjs.converters.save_keras_model(model, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(data, k):\n",
    "    s = slice(k*BATCH_SIZE, (k+1)*BATCH_SIZE)\n",
    "    batch = data.normalized_validation[s]\n",
    "    prediction = model.predict(batch)\n",
    "    [actual_niqqud, actual_dagesh, actual_sin] = [dataset.from_categorical(prediction[0]), dataset.from_categorical(prediction[1]), dataset.from_categorical(prediction[2])]\n",
    "    [expected_niqqud, expected_dagesh, expected_sin] = [data.niqqud_validation[s], data.dagesh_validation[s], data.sin_validation[s]]\n",
    "    actual = data.merge(batch, ns=actual_niqqud, ds=actual_dagesh, ss=actual_sin)\n",
    "    expected = data.merge(batch, ns=expected_niqqud, ds=expected_dagesh, ss=expected_sin)\n",
    "    for i, (a, e) in enumerate(zip(actual, expected)):\n",
    "        print('מצוי: ', a)\n",
    "        print('רצוי: ', e)\n",
    "        print()\n",
    "\n",
    "print_predictions(data_modern, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(os.sep.join([tempfile.gettempdir(), '.tensorboard-info']), ignore_errors=True)\n",
    "shutil.rmtree('logs', ignore_errors=True)\n",
    "os.makedirs('logs')\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = layers.Input((2, 1))\n",
    "layer = layers.Bidirectional(layers.GRU(units=50, return_sequences=True))(input)\n",
    "output = layers.Dense(1, activation='sigmoid')(layer)\n",
    "model = tf.keras.Model(inputs=[input], outputs=[output])\n",
    "\n",
    "lr = 1\n",
    "for i in range(4):\n",
    "    lr /= 3\n",
    "    x = [[[np.random.random()], [np.random.random()]] for _ in range(100000)]\n",
    "    y = [[[a], [a]] for [[a], [_]] in x]\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=\"mean_squared_error\")\n",
    "    model.fit(x, y, epochs=1, verbose=1)\n",
    "print(model.predict([[[1], [0.5]]]))\n",
    "print(model.evaluate([[[1], [0.5]]], [[[1], [1]]]))\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
