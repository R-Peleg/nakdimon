{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import dataset\n",
    "import schedulers\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 380)    16720       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 380)    2313440     embedding_1[0][0]                \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, 380)    0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 16)     6096        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, None, 3)      1143        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 4)      1524        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "N (Softmax)                     (None, None, 16)     0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "D (Softmax)                     (None, None, 3)      0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "S (Softmax)                     (None, None, 4)      0           dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,338,923\n",
      "Trainable params: 2,338,923\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "LETTERS_SIZE = len(dataset.letters_table)\n",
    "NIQQUD_SIZE = len(dataset.niqqud_table)\n",
    "DAGESH_SIZE = len(dataset.dagesh_table)\n",
    "SIN_SIZE = len(dataset.sin_table)\n",
    "\n",
    "def build_model(UNITS=380):  # EMBED_DIM=28\n",
    "    inp = keras.Input(batch_shape=(None, None), batch_size=BATCH_SIZE)\n",
    "    layer = layers.Embedding(LETTERS_SIZE, UNITS, mask_zero=True)(inp)\n",
    "    \n",
    "    bidi = layers.Bidirectional(layers.LSTM(UNITS, return_sequences=True), merge_mode='sum')\n",
    "    layer = bidi(layer)\n",
    "    layer = layers.add([layer, bidi(layer)])\n",
    "\n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(layer)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(layer)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(layer)),\n",
    "    ]\n",
    "    model = keras.Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "model.summary()\n",
    "model.save_weights('./checkpoints/uninit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    K = keras.backend\n",
    "    f = K.floatx()\n",
    "    # convert dense predictions to labels\n",
    "    y_pred_labels =  K.cast(K.argmax(y_pred, axis=-1), f)\n",
    "    \n",
    "    res = K.cast(K.equal(y_true, y_pred_labels), f)\n",
    "    return K.sum(res) / K.sum(K.cast(K.not_equal(y_true, 0), f))\n",
    "\n",
    "\n",
    "def fit(train_validation, scheduler=None, verbose=1, lr=1e-4):\n",
    "    train, valid = train_validation\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=lr), metrics=[accuracy])\n",
    "    callbacks = []\n",
    "    if isinstance(scheduler, schedulers.CircularLearningRate):\n",
    "        scheduler.set_dataset(train, BATCH_SIZE)\n",
    "    if scheduler:\n",
    "        callbacks.append(scheduler)\n",
    "        \n",
    "    x  = train.normalized\n",
    "    vx = valid.normalized\n",
    "    \n",
    "    y  = {'N': train.niqqud, 'D': train.dagesh, 'S': train.sin }\n",
    "    vy = {'N': valid.niqqud, 'D': valid.dagesh, 'S': valid.sin }\n",
    "    \n",
    "    return model.fit(x, y, validation_data=(vx, vy), batch_size=BATCH_SIZE, epochs=1, verbose=verbose, callbacks=callbacks)\n",
    "\n",
    "\n",
    "MAXLEN = 64\n",
    "def load_data(source, maxlen=MAXLEN, validation=0.1):\n",
    "    filenames = [os.path.join('texts', f) for f in source]\n",
    "    train, valid = dataset.load_data(filenames, validation, maxlen=maxlen)\n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mix = load_data(['poetry', 'rabanit', 'pre_modern'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4415/4415 [==============================] - 273s 62ms/step - loss: 0.3155 - N_loss: 0.2434 - D_loss: 0.0634 - S_loss: 0.0087 - N_accuracy: 0.9148 - D_accuracy: 0.9754 - S_accuracy: 0.9977 - val_loss: 0.3062 - val_N_loss: 0.2383 - val_D_loss: 0.0610 - val_S_loss: 0.0069 - val_N_accuracy: 0.9219 - val_D_accuracy: 0.9763 - val_S_accuracy: 0.9982\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/uninit')\n",
    "history = fit(data_mix, scheduler=schedulers.CircularLearningRate(30e-4, 80e-4, 1e-4))\n",
    "model.save_weights('./checkpoints/mix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modern = load_data(validation=0.1, source=['modern'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.9588528871536255\n",
      "1 : 0.9590473175048828\n",
      "2 : 0.9547316431999207\n",
      "3 : 0.9591906070709229\n",
      "4 : 0.961001992225647\n",
      "5 : 0.9616599678993225\n",
      "6 : 0.9608495235443115\n",
      "7 : 0.9570682048797607\n",
      "8 : 0.9559904932975769\n",
      "9 : 0.9598931670188904\n",
      "10 : 0.9606908559799194\n",
      "11 : 0.9620185494422913\n",
      "12 : 0.9541298151016235\n",
      "13 : 0.9578877687454224\n",
      "14 : 0.9566827416419983\n",
      "0.9586463689804077\n"
     ]
    }
   ],
   "source": [
    "vals = []\n",
    "for i in range(15):\n",
    "    model.load_weights('./checkpoints/mix')\n",
    "    data_modern = load_data(validation=0.1, source=['modern'])\n",
    "    history = fit(data_modern, scheduler=schedulers.CircularLearningRate(50e-4, 60e-4, 1e-5), verbose=False)  #  EMBED_DIM=28, UNITS=253: val_N_accuracy: 0.9575 - val_D_accuracy: 0.9856 - val_S_accuracy: 0.9994\n",
    "    val = history.history[\"val_N_accuracy\"][0]\n",
    "    print(i, \":\", val)\n",
    "    vals.append(val)\n",
    "print(np.mean(vals))\n",
    "model.save_weights('./checkpoints/modern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./checkpoints/modern')\n",
    "model.save('modern.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\elaza\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflowjs\\converters\\keras_h5_conversion.py:122: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  return h5py.File(h5file)\n"
     ]
    }
   ],
   "source": [
    "import tensorflowjs as tfjs\n",
    "# model.load_weights('./checkpoints/modern')\n",
    "tfjs.converters.save_keras_model(model, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('./checkpoints/modern')\n",
    "\n",
    "def print_predictions(data, s):\n",
    "    batch = data.normalized[s]\n",
    "    prediction = model.predict(batch)\n",
    "    [actual_niqqud, actual_dagesh, actual_sin] = [dataset.from_categorical(prediction[0]), dataset.from_categorical(prediction[1]), dataset.from_categorical(prediction[2])]\n",
    "    [expected_niqqud, expected_dagesh, expected_sin] = [data.niqqud[s], data.dagesh[s], data.sin[s]]\n",
    "    actual = dataset.merge(data.text[s], ts=batch, ns=actual_niqqud, ds=actual_dagesh, ss=actual_sin)\n",
    "    expected = dataset.merge(data.text[s], ts=batch, ns=expected_niqqud, ds=expected_dagesh, ss=expected_sin)\n",
    "    total = []\n",
    "    for i, (a, e) in enumerate(zip(actual, expected)):\n",
    "        print('מצוי: ', a)\n",
    "        print('רצוי: ', e)\n",
    "        last = expected_niqqud[i].tolist().index(0)\n",
    "        res = expected_niqqud[i][:last] == actual_niqqud[i][:last]\n",
    "        total.extend(res)\n",
    "        print(round(np.mean(res), 2), f'({last - sum(res)} out of {last})')\n",
    "        print()\n",
    "    print(round(np.mean(total), 3))\n",
    "\n",
    "print_predictions(data_modern[1], slice(0, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    model.load_weights('./checkpoints/pre_modern')\n",
    "    p1 = np.exp(np.random.uniform(low=np.log(1e-5), high=np.log(1e-2)))\n",
    "    p2 = np.exp(np.random.uniform(low=np.log(1e-4), high=np.log(1e-1)))\n",
    "    p3 = np.exp(np.random.uniform(low=np.log(1e-5), high=np.log(1e-2)))\n",
    "    print(p1, p2, p3, end=', ', sep=', ')\n",
    "    history = fit(data_modern, scheduler=schedulers.CircularLearningRate(p1, p2, p3), verbose=0)\n",
    "    print(history.history['val_N_accuracy'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "for n, v in enumerate(['accuracy', 'loss'], 0):\n",
    "    for n1, t in enumerate(['D', 'N'], 0):\n",
    "        p = ax[n][n1]\n",
    "        p.plot(history.history[t + '_' + v][0:])\n",
    "        p.plot(history.history['val_' + t + '_' +  v][0:])\n",
    "        p.legend([t + '_Train', t + '_Test'], loc='center right')\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit9bb923b013d04c19b7222e7ae44d4e24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
