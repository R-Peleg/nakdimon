{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import dataset\n",
    "import schedulers\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "assert tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 64, 500)      22000       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 64, 500)      4004000     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 64, 500)      4004000     bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 64, 500)      0           bidirectional[0][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64, 500)      2000        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64, 500)      250500      batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 64, 500)      0           embedding[0][0]                  \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64, 16)       8016        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64, 3)        1503        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 64, 4)        2004        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "N (Softmax)                     (None, 64, 16)       0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "D (Softmax)                     (None, 64, 3)        0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "S (Softmax)                     (None, 64, 4)        0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,294,023\n",
      "Trainable params: 8,293,023\n",
      "Non-trainable params: 1,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "LETTERS_SIZE = len(dataset.letters_table)\n",
    "NIQQUD_SIZE = len(dataset.niqqud_table)\n",
    "DAGESH_SIZE = len(dataset.dagesh_table)\n",
    "SIN_SIZE = len(dataset.sin_table)\n",
    "\n",
    "def build_model(UNITS=500):\n",
    "    inp = keras.Input(shape=(64,), batch_size=None)\n",
    "    embed = layers.Embedding(LETTERS_SIZE, UNITS, mask_zero=True)(inp)\n",
    "    \n",
    "    layer = layers.Bidirectional(layers.LSTM(UNITS, return_sequences=True), merge_mode='sum')(embed)\n",
    "    layer = layers.add([layer, layers.Bidirectional(layers.LSTM(UNITS, return_sequences=True), merge_mode='sum')(layer)])\n",
    "    layer = layers.BatchNormalization()(layer)\n",
    "    layer = layers.add([embed, layers.Dense(UNITS, activation='relu')(layer)])\n",
    "\n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(layer)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(layer)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(layer)),\n",
    "    ]\n",
    "    model = keras.Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "model.summary()\n",
    "model.save_weights('./checkpoints/uninit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# masked version of accuracy and sce\n",
    "def accuracy(real, pred):\n",
    "    acc = tf.keras.metrics.sparse_categorical_accuracy(real, pred)\n",
    "\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(real, 0)), dtype=acc.dtype)\n",
    "    acc *= mask\n",
    "\n",
    "    return tf.reduce_sum(acc) / tf.reduce_sum(mask)\n",
    "\n",
    "def sparse_categorical_crossentropy(y_true, y_pred, sample_weight=None):\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true, 0)), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask) \n",
    "\n",
    "def fit(train_validation, scheduler=None, verbose=1, lr=1e-4, epochs=1):\n",
    "    train, valid = train_validation\n",
    "    \n",
    "    model.compile(loss=sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                  metrics={'N': accuracy, 'D': accuracy, 'S': accuracy})\n",
    "    callbacks = []\n",
    "    if isinstance(scheduler, schedulers.CircularLearningRate):\n",
    "        scheduler.set_dataset(train, BATCH_SIZE)\n",
    "    if scheduler:\n",
    "        callbacks.append(scheduler)    \n",
    "    \n",
    "    x  = train.normalized\n",
    "    y  = {'N': train.niqqud, 'D': train.dagesh, 'S': train.sin }\n",
    "    \n",
    "    if valid is None:\n",
    "        return model.fit(x, y, batch_size=BATCH_SIZE, epochs=epochs, verbose=verbose, callbacks=callbacks)\n",
    "    \n",
    "    vx = valid.normalized\n",
    "    vy = {'N': valid.niqqud, 'D': valid.dagesh, 'S': valid.sin }\n",
    "    \n",
    "    return model.fit(x, y, validation_data=(vx, vy), batch_size=BATCH_SIZE, epochs=epochs, verbose=verbose, callbacks=callbacks)\n",
    "\n",
    "\n",
    "MAXLEN = 64\n",
    "def load_data(source, maxlen=MAXLEN, validation=0.1):\n",
    "    filenames = [os.path.join('texts', f) for f in source]\n",
    "    \n",
    "    if validation == 0:\n",
    "        corpus = dataset.read_corpus(filenames, maxlen)\n",
    "        train = dataset.Data.concatenate(corpus)\n",
    "        train.shuffle()\n",
    "        return train, None\n",
    "    \n",
    "    train, valid = dataset.load_data(filenames, validation, maxlen=maxlen)\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "def load_test_data(maxlen=MAXLEN):\n",
    "    return dataset.load_file('test/ModernTestCorpus-HebrewWiki1-simple.txt', maxlen) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mix = load_data(['poetry', 'rabanit', 'pre_modern'], validation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modern = load_data(['modern'], validation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2426/2426 [==============================] - 244s 101ms/step - loss: 0.2524 - N_loss: 0.1936 - D_loss: 0.0525 - S_loss: 0.0063 - N_accuracy: 0.9318 - D_accuracy: 0.9790 - S_accuracy: 0.99830s - loss: 0.2 - ETA: 0s - loss: 0.2525 - N_loss: 0.1936 - D_loss: 0.0525 - S_loss: 0.0063 - N_accuracy: 0.9317 - D_accuracy: 0.9790 - S_accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/uninit')\n",
    "history = fit(data_mix, scheduler=schedulers.CircularLearningRate(30e-4, 80e-4, 1e-4), epochs=1)\n",
    "model.save_weights('./checkpoints/mix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292/292 [==============================] - 29s 101ms/step - loss: 0.1444 - N_loss: 0.1078 - D_loss: 0.0346 - S_loss: 0.0021 - N_accuracy: 0.9636 - D_accuracy: 0.9869 - S_accuracy: 0.999517s - loss: 0.1814 - N_\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/mix')\n",
    "history = fit(data_modern, scheduler=schedulers.CircularLearningRate(50e-4, 50e-4, 1e-5))\n",
    "model.save_weights('./checkpoints/modern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292/292 [==============================] - 29s 100ms/step - loss: 0.0560 - N_loss: 0.0398 - D_loss: 0.0153 - S_loss: 8.2145e-04 - N_accuracy: 0.9861 - D_accuracy: 0.9941 - S_accuracy: 0.99981s - loss: 0.0562 - N_loss: 0.0400 - D_loss: 0.0154 - S_loss: 8.1836e-04 - N_accuracy: 0.9860 - D_accuracy: 0.9941 - S\n",
      "292/292 [==============================] - 29s 101ms/step - loss: 0.0289 - N_loss: 0.0197 - D_loss: 0.0088 - S_loss: 4.0045e-04 - N_accuracy: 0.9936 - D_accuracy: 0.9968 - S_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/modern')\n",
    "history = fit(data_modern, scheduler=schedulers.CircularLearningRate(5e-4, 15e-4, 1e-5), epochs=1)\n",
    "history = fit(data_modern, scheduler=schedulers.CircularLearningRate(5e-5, 1e-4, 1e-5), epochs=1)\n",
    "model.save_weights('./checkpoints/modern_over')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./checkpoints/modern_over')\n",
    "\n",
    "model.compile()\n",
    "model.save('modern.h5')\n",
    "tfjs.converters.save_keras_model(model, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    model.load_weights('./checkpoints/pre_modern')\n",
    "    p1 = np.exp(np.random.uniform(low=np.log(1e-5), high=np.log(1e-2)))\n",
    "    p2 = np.exp(np.random.uniform(low=np.log(1e-4), high=np.log(1e-1)))\n",
    "    p3 = np.exp(np.random.uniform(low=np.log(1e-5), high=np.log(1e-2)))\n",
    "    print(p1, p2, p3, end=', ', sep=', ')\n",
    "    history = fit(data_modern, scheduler=schedulers.CircularLearningRate(p1, p2, p3), verbose=0)\n",
    "    print(history.history['val_N_accuracy'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "\n",
    "for n, v in enumerate(['accuracy', 'loss'], 0):\n",
    "    for n1, t in enumerate(['N', 'D', 'S'], 0):\n",
    "        p = ax[n][n1]\n",
    "        p.plot(history.history[t + '_' + v][0:])\n",
    "        p.plot(history.history['val_' + t + '_' +  v][0:])\n",
    "        p.legend([t + '_Train', t + '_Test'], loc='center right')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 37ms/step - loss: 0.2230 - N_loss: 0.1833 - D_loss: 0.0366 - S_loss: 0.0031 - N_accuracy: 0.9524 - D_accuracy: 0.9881 - S_accuracy: 0.9992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2229769080877304,\n",
       " 0.18329156935214996,\n",
       " 0.03662189096212387,\n",
       " 0.003063462907448411,\n",
       " 0.9523879289627075,\n",
       " 0.9881359338760376,\n",
       " 0.9991891980171204]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/modern_over')\n",
    "test = load_test_data()\n",
    "x = test.normalized\n",
    "y = {'N': test.niqqud, 'D': test.dagesh, 'S': test.sin }\n",
    "\n",
    "model.compile(loss=sparse_categorical_crossentropy,\n",
    "              metrics={'N': accuracy, 'D': accuracy, 'S': accuracy})\n",
    "\n",
    "model.evaluate(x=x, y=y, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.5%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/modern_over')\n",
    "\n",
    "def real_evaluation(data, s=slice(0, None), print_comparison=True):\n",
    "    batch = data.normalized[s]\n",
    "    prediction = model.predict(batch)\n",
    "    [actual_niqqud, actual_dagesh, actual_sin] = [dataset.from_categorical(prediction[0]), dataset.from_categorical(prediction[1]), dataset.from_categorical(prediction[2])]\n",
    "    [expected_niqqud, expected_dagesh, expected_sin] = [data.niqqud[s], data.dagesh[s], data.sin[s]]\n",
    "    actual = dataset.merge(data.text[s], ts=batch, ns=actual_niqqud, ds=actual_dagesh, ss=actual_sin)\n",
    "    expected = dataset.merge(data.text[s], ts=batch, ns=expected_niqqud, ds=expected_dagesh, ss=expected_sin)\n",
    "    total = []\n",
    "    for i, (b, a, e) in enumerate(zip(batch, actual, expected)):\n",
    "        res = ( (expected_niqqud[i][b > 16] == actual_niqqud[i][b > 16])\n",
    "              & (expected_dagesh[i][b > 16] == actual_dagesh[i][b > 16])\n",
    "              & (expected_sin[i][b > 16] == actual_sin[i][b > 16]) )\n",
    "        total.extend(res)\n",
    "        if print_comparison:\n",
    "            print('מצוי: ', a)\n",
    "            print('רצוי: ', e)\n",
    "            print(f'{np.mean(res):.1%} ({len(res)-np.sum(res)} out of {len(res)})')\n",
    "            print()\n",
    "    print(f'{np.mean(total):.1%}')\n",
    "\n",
    "real_evaluation(test, print_comparison=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit9bb923b013d04c19b7222e7ae44d4e24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
