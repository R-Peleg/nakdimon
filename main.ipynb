{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "import dataset\n",
    "import schedulers\n",
    "\n",
    "assert tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LETTERS_SIZE = len(dataset.letters_table)\n",
    "NIQQUD_SIZE = len(dataset.niqqud_table)\n",
    "DAGESH_SIZE = len(dataset.dagesh_table)\n",
    "SIN_SIZE = len(dataset.sin_table)\n",
    "\n",
    "def build_model(units=500, maxlen=64):\n",
    "    inp = keras.Input(shape=(maxlen,), batch_size=None)\n",
    "    embed = layers.Embedding(LETTERS_SIZE, units, mask_zero=True)(inp)\n",
    "    \n",
    "    layer = layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(embed)\n",
    "    layer = layers.add([layer, layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(layer)])\n",
    "    layer = layers.BatchNormalization()(layer)\n",
    "    layer = layers.add([embed, layers.Dense(units, activation='relu')(layer)])\n",
    "\n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(layer)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(layer)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(layer)),\n",
    "    ]\n",
    "    model = keras.Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# masked version of accuracy and sce\n",
    "def accuracy(real, pred):\n",
    "    acc = tf.keras.metrics.sparse_categorical_accuracy(real, pred)\n",
    "\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(real, 0)), dtype=acc.dtype)\n",
    "    acc *= mask\n",
    "\n",
    "    return tf.reduce_sum(acc) / tf.reduce_sum(mask)\n",
    "\n",
    "def sparse_categorical_crossentropy(y_true, y_pred, sample_weight=None):\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(y_true, 0)), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask) \n",
    "\n",
    "def get_xy(d):\n",
    "    if d is None:\n",
    "        return None\n",
    "    x = d.normalized\n",
    "    y = {'N': d.niqqud, 'D': d.dagesh, 'S': d.sin }\n",
    "    return (x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAXLEN = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "data = {}\n",
    "data['mix'] = dataset.load_data([\n",
    "    'hebrew_diacritized_private/poetry',\n",
    "    'hebrew_diacritized_private/rabanit',\n",
    "    'hebrew_diacritized_private/pre_modern'], validation_rate=0, maxlen=MAXLEN)\n",
    "\n",
    "data['modern'] = dataset.load_data([\n",
    "    'hebrew_diacritized/modern'], validation_rate=0, maxlen=MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_MODE=dryrun\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            Using <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> in dryrun mode. Not logging results to the cloud.<br/>\n",
       "            Call wandb.login() to authenticate this machine.<br/>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2410/2410 [==============================] - 246s 102ms/step - loss: 0.5250 - N_loss: 0.2957 - D_loss: 0.1168 - S_loss: 0.1126 - N_accuracy: 0.8963 - D_accuracy: 0.9525 - S_accuracy: 0.9689 3:49 - loss: 1.9619 - N_loss: 1.2121 - D_loss: 0.4219 - S_loss: 0.3279 - N_accuracy: 0.5639 - D_accuracy: 0.8065 - - ETA: 3:47 - loss: 1.8416 - N_l - ETA: 3:38 - loss: 1.4407 - N_loss: 0.8800 - D_loss: 0.3063 - S_loss: 0.2544 - N_accuracy: 0.6810 - D_accuracy: 0.8 - ETA: 3:36 - loss: 1.3827 - N_loss: 0.8424 - D_loss: 0.2946 - S_loss: 0.2458 - N_accuracy: 0.6947 - D_accuracy - ETA: 3:33 - loss: 1.3209 - N_loss: 0.8032 - D_loss: 0.2819 - S_loss: 0.2358 - N_accuracy: 0.7092 - D_accuracy: 0.8748 - S_accuracy: 0 - ETA: 2:52  - ETA: 2:39 - loss: 0.8213 - N_loss: 0.4826 - D_loss: 0.1789 - S_loss: 0.1598 - N_accuracy: 0.8275 - D_accuracy: 0.9243 - S_ - ETA: 2:38 - loss: 0.8147 - N_loss: 0.4785 - D_loss: 0.1775 - S_lo - ETA: 2:30 - loss: 0.7877 - N_loss: 0.4610 - D_loss: 0.1720 - S_loss: 0.1547 - N_accuracy: 0.8355 - D_accuracy: 0.9275 - S_accuracy: 0 - ETA: 2:30 - loss: 0. - ETA: 2:18 - loss: 0.7500 - N_loss: 0.4366 - D_loss: 0.1640 - S_loss: 0.1493 - N_accuracy: 0.8445 - D_acc - ETA: 2:15 - loss: 0.7400 - N_loss: 0.4306 - D_loss: 0.1620 - S_loss: 0.1474 - N_accuracy: 0.8468 - D_accuracy: 0.9321 - S_accura - ETA: 2 - ETA: 1:32 - loss: 0.6541 - N_loss: 0.3752 - D_loss: 0.14 - ETA: 5s - loss: 0.5312 - N_loss: 0.2993 - D_loss: 0.1180 - S_loss: 0.1138 - N_accuracy: 0.8950 - D_accuracy: 0.9519  - ETA: 3s - loss: 0.5288 - N_loss: 0.2979 - D_loss: 0.1175 - S_loss: 0.1133 - N_accuracy: 0.8955 - D_ac\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_MODE dryrun\n",
    "maxlen = MAXLEN\n",
    "\n",
    "def experiment(architecture):\n",
    "    UNITS=500\n",
    "    np.random.seed(2)\n",
    "    model = architecture(units=UNITS)\n",
    "    model.compile(loss=sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                  metrics={'N': accuracy, 'D': accuracy, 'S': accuracy})\n",
    "\n",
    "    model.save_weights('./checkpoints/uninit')\n",
    "    \n",
    "    \n",
    "    config = {\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'maxlen': MAXLEN,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'units': UNITS,\n",
    "        'model': model,\n",
    "        'order': [\n",
    "            ('mix',    [(30e-4, 80e-4, 1e-4)], 'mix'),\n",
    "            ('modern', [(50e-4, 50e-4, 1e-5)], 'modern'),\n",
    "            ('modern', [(50e-4, 50e-4, 1e-5),\n",
    "                        #(50e-4, 50e-4, 1e-5),\n",
    "                       ], 'modern_over'),\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    run = wandb.init(project=\"dotter\",\n",
    "                     group=\"architecture\",\n",
    "                     name=f'model_{architecture.__name__}',\n",
    "                     tags=['CLR', 'ordered'],\n",
    "                     config=config)\n",
    "\n",
    "    with run:\n",
    "        for kind, clrs, save in config['order']:\n",
    "            train, validation = data[kind]\n",
    "\n",
    "            training_data = (x, y) = get_xy(train)\n",
    "            validation_data = get_xy(validation)\n",
    "\n",
    "            wandb_callback = WandbCallback(log_batch_frequency=10, training_data=training_data, validation_data=validation_data,\n",
    "                                           log_weights=True)\n",
    "\n",
    "            for clr in clrs:\n",
    "                scheduler = schedulers.CircularLearningRate(*clr)\n",
    "                scheduler.set_dataset(train, BATCH_SIZE)\n",
    "                callbacks = [wandb_callback, scheduler]\n",
    "                history = model.fit(x, y, validation_data=validation_data, batch_size=BATCH_SIZE, verbose=1, callbacks=callbacks)\n",
    "            model.save(os.path.join(wandb.run.dir, save + \".h5\"))\n",
    "            model.save_weights('./checkpoints/' + save)\n",
    "            return model\n",
    "\n",
    "def full(units):\n",
    "    inp = keras.Input(shape=(maxlen,), batch_size=None)\n",
    "    embed = layers.Embedding(LETTERS_SIZE, units, mask_zero=True)(inp)\n",
    "    \n",
    "    layer = layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(embed)\n",
    "    layer = layers.add([layer, layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(layer)])\n",
    "    layer = layers.BatchNormalization()(layer)\n",
    "    layer = layers.add([embed, layers.Dense(units, activation='relu')(layer)])\n",
    "\n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(layer)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(layer)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(layer)),\n",
    "    ]\n",
    "    return keras.Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "def no_first_add(units):\n",
    "    inp = keras.Input(shape=(maxlen,), batch_size=None)\n",
    "    embed = layers.Embedding(LETTERS_SIZE, units, mask_zero=True)(inp)\n",
    "    \n",
    "    layer = layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(embed)\n",
    "    layer = layers.BatchNormalization()(layer)\n",
    "    layer = layers.Dense(units, activation='relu')(layer)\n",
    "\n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(layer)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(layer)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(layer)),\n",
    "    ]\n",
    "    return keras.Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "def share_weights(units):\n",
    "    inp = keras.Input(shape=(maxlen,), batch_size=None)\n",
    "    embed = layers.Embedding(LETTERS_SIZE, units, mask_zero=True)(inp)\n",
    "    \n",
    "    lstm = layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')\n",
    "    layer = lstm(embed)\n",
    "    layer = layers.add([layer, lstm(layer)])\n",
    "    layer = layers.BatchNormalization()(layer)\n",
    "    layer = layers.add([embed, layers.Dense(units, activation='relu')(layer)])\n",
    "\n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(layer)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(layer)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(layer)),\n",
    "    ]\n",
    "    return keras.Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "def no_batchnorm(units):\n",
    "    inp = keras.Input(shape=(maxlen,), batch_size=None)\n",
    "    embed = layers.Embedding(LETTERS_SIZE, units, mask_zero=True)(inp)\n",
    "    \n",
    "    layer = layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(embed)\n",
    "    layer = layers.add([layer, layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(layer)])\n",
    "    layer = layers.add([embed, layers.Dense(units, activation='relu')(layer)])\n",
    "\n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(layer)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(layer)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(layer)),\n",
    "    ]\n",
    "    return keras.Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "def no_last_add(units):\n",
    "    inp = keras.Input(shape=(maxlen,), batch_size=None)\n",
    "    embed = layers.Embedding(LETTERS_SIZE, units, mask_zero=True)(inp)\n",
    "    \n",
    "    layer = layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(embed)\n",
    "    layer = layers.add([layer, layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(layer)])\n",
    "    layer = layers.BatchNormalization()(layer)\n",
    "    layer = layers.Dense(units, activation='relu')(layer)\n",
    "\n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(layer)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(layer)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(layer)),\n",
    "    ]\n",
    "    return keras.Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "def no_last_dense(units):\n",
    "    inp = keras.Input(shape=(maxlen,), batch_size=None)\n",
    "    embed = layers.Embedding(LETTERS_SIZE, units, mask_zero=True)(inp)\n",
    "    \n",
    "    layer = layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(embed)\n",
    "    layer = layers.add([layer, layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(layer)])\n",
    "    layer = layers.BatchNormalization()(layer)\n",
    "\n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(layer)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(layer)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(layer)),\n",
    "    ]\n",
    "    return keras.Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "def no_batchnorm(units):\n",
    "    inp = keras.Input(shape=(maxlen,), batch_size=None)\n",
    "    embed = layers.Embedding(LETTERS_SIZE, units, mask_zero=True)(inp)\n",
    "    \n",
    "    layer = layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(embed)\n",
    "    layer = layers.add([layer, layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(layer)])\n",
    "    layer = layers.add([embed, layers.Dense(units, activation='relu')(layer)])\n",
    "\n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(layer)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(layer)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(layer)),\n",
    "    ]\n",
    "    return keras.Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "def linear_last_dense(units):\n",
    "    inp = keras.Input(shape=(maxlen,), batch_size=None)\n",
    "    embed = layers.Embedding(LETTERS_SIZE, units, mask_zero=True)(inp)\n",
    "    \n",
    "    layer = layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(embed)\n",
    "    layer = layers.add([layer, layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(layer)])\n",
    "    layer = layers.BatchNormalization()(layer)\n",
    "    layer = layers.add([embed, layers.Dense(units)(layer)])\n",
    "\n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(layer)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(layer)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(layer)),\n",
    "    ]\n",
    "    return keras.Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "def linear_last_dense_no_batchnorm(units):\n",
    "    inp = keras.Input(shape=(maxlen,), batch_size=None)\n",
    "    embed = layers.Embedding(LETTERS_SIZE, units, mask_zero=True)(inp)\n",
    "    \n",
    "    layer = layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(embed)\n",
    "    layer = layers.add([layer, layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(layer)])\n",
    "    layer = layers.add([embed, layers.Dense(units)(layer)])\n",
    "\n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(layer)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(layer)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(layer)),\n",
    "    ]\n",
    "    return keras.Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "def linear_last_dense_no_batchnorm_share_weights(units):\n",
    "    inp = keras.Input(shape=(maxlen,), batch_size=None)\n",
    "    embed = layers.Embedding(LETTERS_SIZE, units, mask_zero=True)(inp)\n",
    "    \n",
    "    lstm = layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')\n",
    "    layer = lstm(embed)\n",
    "    layer = layers.add([layer, lstm(layer)])\n",
    "    layer = layers.add([embed, layers.Dense(units)(layer)])\n",
    "\n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(layer)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(layer)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(layer)),\n",
    "    ]\n",
    "    return keras.Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "def linear_last_dense_no_batchnorm_no_first_add(units):\n",
    "    inp = keras.Input(shape=(maxlen,), batch_size=None)\n",
    "    embed = layers.Embedding(LETTERS_SIZE, units, mask_zero=True)(inp)\n",
    "    \n",
    "    layer = layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(embed)\n",
    "    layer = layers.add([embed, layers.Dense(units)(layer)])\n",
    "\n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(layer)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(layer)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(layer)),\n",
    "    ]\n",
    "    return keras.Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "def linear_last_dense_no_batchnorm_no_last_add(units):\n",
    "    inp = keras.Input(shape=(maxlen,), batch_size=None)\n",
    "    embed = layers.Embedding(LETTERS_SIZE, units, mask_zero=True)(inp)\n",
    "    \n",
    "    layer = layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(embed)\n",
    "    layer = layers.add([layer, layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(layer)])\n",
    "    layer = layers.Dense(units)(layer)\n",
    "    \n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(layer)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(layer)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(layer)),\n",
    "    ]\n",
    "    return keras.Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "def no_batchnorm_no_last_dense(units):\n",
    "    inp = keras.Input(shape=(maxlen,), batch_size=None)\n",
    "    embed = layers.Embedding(LETTERS_SIZE, units, mask_zero=True)(inp)\n",
    "    \n",
    "    layer = layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(embed)\n",
    "    layer = layers.add([layer, layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(layer)])\n",
    "    \n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(layer)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(layer)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(layer)),\n",
    "    ]\n",
    "    return keras.Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "def simple_bilstm(units):\n",
    "    inp = keras.Input(shape=(maxlen,), batch_size=None)\n",
    "    embed = layers.Embedding(LETTERS_SIZE, units, mask_zero=True)(inp)\n",
    "    \n",
    "    layer = layers.Bidirectional(layers.LSTM(units, return_sequences=True), merge_mode='sum')(embed)\n",
    "    \n",
    "    outputs = [\n",
    "        layers.Softmax(name='N')(layers.Dense(NIQQUD_SIZE)(layer)),\n",
    "        layers.Softmax(name='D')(layers.Dense(DAGESH_SIZE)(layer)),\n",
    "        layers.Softmax(name='S')(layers.Dense(SIN_SIZE)(layer)),\n",
    "    ]\n",
    "    return keras.Model(inputs=inp, outputs=outputs)\n",
    "\n",
    "for architecture in [linear_last_dense_no_batchnorm]:\n",
    "    model = experiment(architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./checkpoints/modern_over')\n",
    "\n",
    "model.compile()\n",
    "model.save('modern.h5')\n",
    "tfjs.converters.save_keras_model(model, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=3)\n",
    "\n",
    "for n, v in enumerate(['accuracy', 'loss'], 0):\n",
    "    for n1, t in enumerate(['N', 'D', 'S'], 0):\n",
    "        p = ax[n][n1]\n",
    "        p.plot(history.history[t + '_' + v][0:])\n",
    "        p.plot(history.history['val_' + t + '_' +  v][0:])\n",
    "        p.legend([t + '_Train', t + '_Test'], loc='center right')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "test, _ = dataset.load_data(['test/modernTestCorpus/'], 0, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 39ms/step - loss: 0.4038 - N_loss: 0.2181 - D_loss: 0.0896 - S_loss: 0.0961 - N_accuracy: 0.9291 - D_accuracy: 0.9650 - S_accuracy: 0.9722\n"
     ]
    }
   ],
   "source": [
    "# model = linear_last_dense_no_batchnorm(units=500)\n",
    "# model.load_weights('./checkpoints/modern_over')\n",
    "x = test.normalized\n",
    "y = {'N': test.niqqud, 'D': test.dagesh, 'S': test.sin }\n",
    "\n",
    "model.compile(loss=sparse_categorical_crossentropy,\n",
    "              metrics={'N': accuracy, 'D': accuracy, 'S': accuracy})\n",
    "\n",
    "_ = model.evaluate(x=x, y=y, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letters: 94.88%, words: 77.43%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/modern_over')\n",
    "\n",
    "def real_evaluation(data, s=slice(0, None), print_comparison=True):\n",
    "    batch = data.normalized[s]\n",
    "    prediction = model.predict(batch)\n",
    "    [actual_niqqud, actual_dagesh, actual_sin] = [dataset.from_categorical(prediction[0]), dataset.from_categorical(prediction[1]), dataset.from_categorical(prediction[2])]\n",
    "    [expected_niqqud, expected_dagesh, expected_sin] = [data.niqqud[s], data.dagesh[s], data.sin[s]]\n",
    "    actual = dataset.merge(data.text[s], batch, actual_niqqud, actual_dagesh, actual_sin)\n",
    "    expected = dataset.merge(data.text[s], batch, expected_niqqud, expected_dagesh, expected_sin)\n",
    "    total_letters = []\n",
    "    total_words = []\n",
    "    for i, (b, a, e) in enumerate(zip(batch, actual, expected)):\n",
    "        letters = []\n",
    "        letters.extend(expected_niqqud[i][expected_niqqud[i]>0] == actual_niqqud[i][expected_niqqud[i]>0])\n",
    "        letters.extend(expected_dagesh[i][expected_dagesh[i]>0] == actual_dagesh[i][expected_dagesh[i]>0])\n",
    "        letters.extend(expected_sin[i][expected_sin[i]>0] == actual_sin[i][expected_sin[i]>0])\n",
    "        total_letters.extend(letters)\n",
    "        words = []\n",
    "        for aw, ew in zip(a.split(), e.split()):\n",
    "            if len([x for x in 'אבגדהוזחטיכלמנסעפצקרשתךםןףץ' if x in aw]) > 1:\n",
    "                words.append(aw == ew)\n",
    "                if print_comparison and aw != ew:\n",
    "                    print(aw, ew)\n",
    "        total_words.extend(words)\n",
    "        if print_comparison:\n",
    "            print('מצוי: ', a)\n",
    "            print('רצוי: ', e)\n",
    "            print(f'{np.mean(letters):.2%} ({len(letters)-np.sum(letters)} out of {len(letters)})')\n",
    "            print(f'{np.mean(words):.2%} ({len(words)-np.sum(words)} out of {len(words)})')\n",
    "            print()\n",
    "    print(f'letters: {np.mean(total_letters):.2%}, words: {np.mean(total_words):.2%}')\n",
    "\n",
    "real_evaluation(test, s=slice(0, None), print_comparison=False)  #  letters: 94.88%, words: 77.43%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import hebrew\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'len'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit9bb923b013d04c19b7222e7ae44d4e24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
