{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "assert tf.config.list_physical_devices('GPU')\n",
    "\n",
    "%autoreload\n",
    "import dataset\n",
    "\n",
    "import schedulers\n",
    "\n",
    "%autoreload\n",
    "import transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "MAXLEN = 60\n",
    "\n",
    "LETTERS_SIZE = len(dataset.letters_table)\n",
    "NIQQUD_SIZE = len(dataset.niqqud_table)\n",
    "DAGESH_SIZE = len(dataset.dagesh_table)\n",
    "SIN_SIZE = len(dataset.sin_table)\n",
    "\n",
    "d_model = 1000\n",
    "\n",
    "model = transformer.Transformer(\n",
    "    num_layers=1,\n",
    "    d_model=d_model,\n",
    "    num_heads=1,\n",
    "    dff=256,\n",
    "    input_vocab_size=LETTERS_SIZE,\n",
    "    target_vocab_size=NIQQUD_SIZE, \n",
    "    maximum_position_encoding_input=MAXLEN, \n",
    "    maximum_position_encoding_target=MAXLEN,\n",
    "    rate=0.0\n",
    ")\n",
    "learning_rate = transformer.CustomSchedule(d_model, warmup_steps=1000)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9),\n",
    "    loss=transformer.train_loss\n",
    ")\n",
    "\n",
    "# model.build((None, MAXLEN))\n",
    "# model.summary()\n",
    "model.save_weights('./checkpoints/uninit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(source, validation=0.1):\n",
    "    filenames = [os.path.join('texts', f) for f in source]\n",
    "    train, valid = dataset.load_data(filenames, validation, maxlen=MAXLEN)\n",
    "    return train, valid\n",
    "\n",
    "def fit(data):\n",
    "    transformer.train_loss.reset_states()\n",
    "    transformer.train_accuracy.reset_states()\n",
    "\n",
    "    for i in range(len(data[0])):\n",
    "        s = slice(i, (i+1))\n",
    "        res = model.train_step(data[0].normalized[s], data[0].niqqud[s])\n",
    "        print(i, {k:round(v, 4) for k, v in res.items()}, end='      \\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17826 {'loss': 1.5824, 'acc': 0.4748}loss': 2.1412, 'acc': 0.3864} {'loss': 2.0333, 'acc': 0.4071} {'loss': 1.7163, 'acc': 0.4662} {'loss': 1.7141, 'acc': 0.466} {'loss': 1.6958, 'acc': 0.4669} {'loss': 1.69, 'acc': 0.4674} {'loss': 1.6893, 'acc': 0.4674} {'loss': 1.6732, 'acc': 0.4691} {'loss': 1.6717, 'acc': 0.4691} {'loss': 1.664, 'acc': 0.4698} {'loss': 1.6617, 'acc': 0.47} {'loss': 1.6438, 'acc': 0.4709} {'loss': 1.6414, 'acc': 0.4712} {'loss': 1.6383, 'acc': 0.4709} {'loss': 1.6331, 'acc': 0.4711} {'loss': 1.6323, 'acc': 0.4712} {'loss': 1.6322, 'acc': 0.4711} {'loss': 1.6279, 'acc': 0.4715} {'loss': 1.6262, 'acc': 0.4718} {'loss': 1.6229, 'acc': 0.472} {'loss': 1.6226, 'acc': 0.472} {'loss': 1.6201, 'acc': 0.4722} {'loss': 1.6151, 'acc': 0.4724} {'loss': 1.615, 'acc': 0.4724} {'loss': 1.6121, 'acc': 0.4724} {'loss': 1.612, 'acc': 0.4725} {'loss': 1.6116, 'acc': 0.4725} {'loss': 1.6101, 'acc': 0.4727} {'loss': 1.6098, 'acc': 0.4727} {'loss': 1.608, 'acc': 0.4729} {'loss': 1.6079, 'acc': 0.4729} {'loss': 1.6069, 'acc': 0.473} {'loss': 1.6068, 'acc': 0.473} {'loss': 1.6044, 'acc': 0.4731} {'loss': 1.6044, 'acc': 0.4731} {'loss': 1.603, 'acc': 0.4733} {'loss': 1.603, 'acc': 0.4733} {'loss': 1.6014, 'acc': 0.4735} {'loss': 1.6009, 'acc': 0.4735} {'loss': 1.6006, 'acc': 0.4735} {'loss': 1.6004, 'acc': 0.4735} {'loss': 1.6, 'acc': 0.4736} {'loss': 1.5995, 'acc': 0.4737} {'loss': 1.5987, 'acc': 0.4737} {'loss': 1.5981, 'acc': 0.4738} {'loss': 1.5972, 'acc': 0.4739} {'loss': 1.5968, 'acc': 0.474} {'loss': 1.5968, 'acc': 0.474} {'loss': 1.5967, 'acc': 0.474} {'loss': 1.5967, 'acc': 0.474} {'loss': 1.5962, 'acc': 0.474} {'loss': 1.5958, 'acc': 0.474} {'loss': 1.5942, 'acc': 0.4742} {'loss': 1.5939, 'acc': 0.4742} {'loss': 1.5923, 'acc': 0.4743} {'loss': 1.5917, 'acc': 0.4743} {'loss': 1.5915, 'acc': 0.4744} {'loss': 1.5914, 'acc': 0.4744} {'loss': 1.5903, 'acc': 0.4743} {'loss': 1.5901, 'acc': 0.4744} {'loss': 1.5898, 'acc': 0.4743} {'loss': 1.5896, 'acc': 0.4744} {'loss': 1.5895, 'acc': 0.4744} {'loss': 1.5889, 'acc': 0.4744} {'loss': 1.5881, 'acc': 0.4745} {'loss': 1.5881, 'acc': 0.4745} {'loss': 1.588, 'acc': 0.4745} {'loss': 1.588, 'acc': 0.4744} {'loss': 1.5868, 'acc': 0.4746} {'loss': 1.5861, 'acc': 0.4747} {'loss': 1.5857, 'acc': 0.4747} {'loss': 1.5854, 'acc': 0.4748} {'loss': 1.5854, 'acc': 0.4747} {'loss': 1.5854, 'acc': 0.4747} {'loss': 1.5853, 'acc': 0.4747} {'loss': 1.5853, 'acc': 0.4747} {'loss': 1.5853, 'acc': 0.4747} {'loss': 1.5853, 'acc': 0.4747} {'loss': 1.5853, 'acc': 0.4747} {'loss': 1.5852, 'acc': 0.4747} {'loss': 1.5852, 'acc': 0.4747} {'loss': 1.5852, 'acc': 0.4747} {'loss': 1.5852, 'acc': 0.4747} {'loss': 1.5852, 'acc': 0.4747} {'loss': 1.5852, 'acc': 0.4747} {'loss': 1.5852, 'acc': 0.4747} {'loss': 1.5851, 'acc': 0.4747} {'loss': 1.5851, 'acc': 0.4747} {'loss': 1.5851, 'acc': 0.4747} {'loss': 1.5851, 'acc': 0.4747} {'loss': 1.5851, 'acc': 0.4747} {'loss': 1.5851, 'acc': 0.4747} {'loss': 1.5851, 'acc': 0.4747} {'loss': 1.585, 'acc': 0.4747} {'loss': 1.585, 'acc': 0.4747} {'loss': 1.585, 'acc': 0.4747} {'loss': 1.585, 'acc': 0.4747} {'loss': 1.585, 'acc': 0.4747} {'loss': 1.585, 'acc': 0.4747} {'loss': 1.585, 'acc': 0.4747} {'loss': 1.585, 'acc': 0.4747} {'loss': 1.5848, 'acc': 0.4747} {'loss': 1.5841, 'acc': 0.4748} {'loss': 1.5841, 'acc': 0.4747} {'loss': 1.584, 'acc': 0.4747} {'loss': 1.5839, 'acc': 0.4747} {'loss': 1.5839, 'acc': 0.4747} {'loss': 1.5836, 'acc': 0.4747} {'loss': 1.5835, 'acc': 0.4747} {'loss': 1.5834, 'acc': 0.4747} {'loss': 1.5834, 'acc': 0.4747} {'loss': 1.5831, 'acc': 0.4747} {'loss': 1.5827, 'acc': 0.4748} {'loss': 1.5825, 'acc': 0.4748}\r"
     ]
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/uninit')\n",
    "history = fit(data_modern)\n",
    "# print(true_accuracy(data_modern))\n",
    "model.save_weights('./checkpoints/modern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "model.load_weights('./checkpoints/modern')\n",
    "\n",
    "def print_predictions(data, s):\n",
    "    batch = data.normalized[s]\n",
    "    prediction = model.predict(batch)\n",
    "    [actual_niqqud, actual_dagesh, actual_sin] = [dataset.from_categorical(prediction[0]), dataset.from_categorical(prediction[1]), dataset.from_categorical(prediction[2])]\n",
    "    [expected_niqqud, expected_dagesh, expected_sin] = [data.niqqud[s], data.dagesh[s], data.sin[s]]\n",
    "    actual = dataset.merge(data.text[s], ts=batch, ns=actual_niqqud, ds=actual_dagesh, ss=actual_sin)\n",
    "    expected = dataset.merge(data.text[s], ts=batch, ns=expected_niqqud, ds=expected_dagesh, ss=expected_sin)\n",
    "    total = []\n",
    "    for i, (a, e) in enumerate(zip(actual, expected)):\n",
    "        print('מצוי: ', a)\n",
    "        print('רצוי: ', e)\n",
    "        last = expected_niqqud[i].tolist().index(0)\n",
    "        res = expected_niqqud[i][:last] == actual_niqqud[i][:last]\n",
    "        total.extend(res)\n",
    "        print(round(np.mean(res), 2), f'({last - sum(res)} out of {last})')\n",
    "        print()\n",
    "    print(round(np.mean(total), 3))\n",
    "\n",
    "print_predictions(data_modern[1], slice(0, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    sentence = tokenizer_pt.encode(sentence)\n",
    "\n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "\n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                            if i < tokenizer_en.vocab_size], \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
