{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "assert tf.config.list_physical_devices('GPU')\n",
    "\n",
    "import collections\n",
    "\n",
    "import dataset\n",
    "\n",
    "%autoreload\n",
    "import transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_49 (Encoder)         multiple                  466862    \n",
      "_________________________________________________________________\n",
      "decoder_49 (Decoder)         multiple                  506234    \n",
      "_________________________________________________________________\n",
      "dense_1601 (Dense)           multiple                  1648      \n",
      "_________________________________________________________________\n",
      "softmax_49 (Softmax)         multiple                  0         \n",
      "=================================================================\n",
      "Total params: 974,744\n",
      "Trainable params: 974,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "MAXLEN = 50\n",
    "\n",
    "LETTERS_SIZE = len(dataset.letters_table)\n",
    "NIQQUD_SIZE = len(dataset.niqqud_table)\n",
    "DAGESH_SIZE = len(dataset.dagesh_table)\n",
    "SIN_SIZE = len(dataset.sin_table)\n",
    "\n",
    "d_model = 102\n",
    "\n",
    "model = transformer.Transformer(\n",
    "    num_layers=1,\n",
    "    d_model=d_model,\n",
    "    num_heads=6,\n",
    "    dff=2048,\n",
    "    input_vocab_size=LETTERS_SIZE,\n",
    "    target_vocab_size=NIQQUD_SIZE, \n",
    "    maximum_position_encoding_input=MAXLEN,\n",
    "    maximum_position_encoding_target=MAXLEN,\n",
    "    rate=0.0\n",
    ")\n",
    "\n",
    "learning_rate = transformer.CustomSchedule(d_model, warmup_steps=3000)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss='sparse_categorical_crossentropy',  # transformer.MaskedCategoricalCrossentropy(),  # tf.keras.losses.sparse_categorical_crossentropy,\n",
    "    metrics=['accuracy']  # unmasked, so incorrect\n",
    ")\n",
    "# pseudo \"build\" step, to allow printing a summary:\n",
    "# model.run_eagerly = True\n",
    "h = model.pseudo_build(MAXLEN, MAXLEN)\n",
    "model.summary()\n",
    "model.save_weights('./checkpoints/uninit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(source, validation=0.1):\n",
    "    filenames = [os.path.join('texts', f) for f in source]\n",
    "    train, valid = dataset.load_data(filenames, validation, maxlen=MAXLEN)\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "def print_items(i, res, total):\n",
    "    out = ' - '.join(f\"{k}: {v:.4f}\" for k, v in res.items())\n",
    "    print(f\"{i:4d}/{total:4d} - {out}\", end='      \\r')\n",
    "\n",
    "    \n",
    "def fit(data, epochs=1):\n",
    "    train, valid = data\n",
    "    total_train = len(train)//BATCH_SIZE\n",
    "    total_valid = len(valid)//BATCH_SIZE\n",
    "    history = collections.defaultdict(list)\n",
    "    for epoch in range(epochs):\n",
    "        model.reset_metrics()\n",
    "        for i in range(total_train):\n",
    "            s = slice(i*BATCH_SIZE, (i+1)*BATCH_SIZE)\n",
    "            padded = np.hstack([np.ones((BATCH_SIZE, 1)), train.niqqud[s]])\n",
    "            res = model.train_step(train.normalized[s], padded)\n",
    "            \n",
    "            out = ' - '.join(f\"{k}: {v:.4f}\" for k, v in res.items())\n",
    "            print(f\"\\r{i:4d}/{total_train:4d} - {out}\", end='')\n",
    "            \n",
    "        model.reset_metrics()\n",
    "        \n",
    "        for i in range(total_valid):\n",
    "            s = slice(i*BATCH_SIZE, (i+1)*BATCH_SIZE)\n",
    "            res = model.test_step(valid.normalized[s], valid.niqqud[s])\n",
    "\n",
    "        out = ''.join(f\" - {k}: {v:.4f}\" for k, v in res.items())\n",
    "        print(out)\n",
    "        \n",
    "        for k, v in res.items():\n",
    "            history[k].append(res[k].numpy())\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_other = load_data(['biblical', 'garbage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mix = load_data(['poetry', 'rabanit', 'pre_modern'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modern = load_data(validation=0.1, source=['modern'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988/1989 - loss: 1.0183 - accuracy: 0.6653 - val_loss: 0.8370 - val_accuracy: 0.7340\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/uninit')\n",
    "history = fit(data_other, epochs=1)\n",
    "model.save_weights('./checkpoints/other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5940/5941 - loss: 0.3877 - accuracy: 0.8654 - val_loss: 3.1124 - val_accuracy: 0.5185\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/other')\n",
    "history = fit(data_mix, epochs=1) # (102, 2048, 6) warmup=270 : 974,746 - 0.8323  (7 - same)\n",
    "model.save_weights('./checkpoints/mix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 683/ 684 - loss: 0.2830 - accuracy: 0.8995"
     ]
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/mix')\n",
    "history = fit(data_modern, epochs=2)\n",
    "# print(true_accuracy(data_modern)\n",
    "model.save_weights('./checkpoints/modern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "model.load_weights('./checkpoints/modern')\n",
    "\n",
    "def print_predictions(data, s):\n",
    "    batch = data.normalized[s]\n",
    "    prediction = model.predict(batch)\n",
    "    [actual_niqqud, actual_dagesh, actual_sin] = [dataset.from_categorical(prediction[0]), dataset.from_categorical(prediction[1]), dataset.from_categorical(prediction[2])]\n",
    "    [expected_niqqud, expected_dagesh, expected_sin] = [data.niqqud[s], data.dagesh[s], data.sin[s]]\n",
    "    actual = dataset.merge(data.text[s], ts=batch, ns=actual_niqqud, ds=actual_dagesh, ss=actual_sin)\n",
    "    expected = dataset.merge(data.text[s], ts=batch, ns=expected_niqqud, ds=expected_dagesh, ss=expected_sin)\n",
    "    total = []\n",
    "    for i, (a, e) in enumerate(zip(actual, expected)):\n",
    "        print('מצוי: ', a)\n",
    "        print('רצוי: ', e)\n",
    "        last = expected_niqqud[i].tolist().index(0)\n",
    "        res = expected_niqqud[i][:last] == actual_niqqud[i][:last]\n",
    "        total.extend(res)\n",
    "        print(f'{np.mean(res):.2f} ({last - sum(res)} out of {last})')\n",
    "        print()\n",
    "    print(round(np.mean(total), 3))\n",
    "\n",
    "print_predictions(data_modern[1], slice(0, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(history))\n",
    "\n",
    "for i, v in enumerate(history.values()):\n",
    "    ax[i].plot(v)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 31, 22, 41, 26, 30, 5, 18, 18, 26, 43, 5, 21, 34, 37, 41, 13, 5, 28, 29, 30, 5, 34, 37, 41, 22, 5, 34, 37, 22, 41, 26, 30, 5, 22, 21, 43, 37, 22, 39, 39, 22, 5, 31, 39, 24, 22, 40, 5, 0]\n",
      "[0, 0, 0, 0, 0, 1, 1, 2, 7, 1, 1, 1, 15, 7, 8, 1, 1, 1, 13, 10, 1, 1, 6, 2, 1, 14, 1, 2, 1, 14, 6, 1, 1, 1, 2, 6, 2, 1, 14, 2, 1, 14, 1, 6, 2, 1, 14, 1, 1, 0]\n",
      "[15, 1, 11, 6, 1, 1, 1, 2, 7, 1, 1, 1, 15, 7, 8, 1, 1, 1, 13, 10, 1, 1, 6, 2, 1, 14, 1, 6, 1, 14, 6, 1, 1, 1, 2, 6, 2, 1, 11, 2, 1, 14, 1, 6, 2, 1, 11, 1, 1, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.508125"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/modern')\n",
    "\n",
    "def predict(x):\n",
    "    batch_len = x.shape[0]\n",
    "    y_probs = tf.ones((batch_len, 1, 16), dtype=tf.float32)\n",
    "\n",
    "    padding_mask = transformer.create_padding_mask(x)\n",
    "    dec_target_padding_mask = transformer.create_padding_mask(x)\n",
    "    timesteps = x.shape[-1]\n",
    "    for i in range(timesteps):\n",
    "        future = tf.ones((batch_len, timesteps - i - 1), dtype=tf.int32)\n",
    "        y_pred = tf.cast(tf.argmax(y_probs, axis=-1), tf.int32)\n",
    "        y_augment = tf.concat([y_pred, future], axis=-1)\n",
    "\n",
    "        predictions, _ = model(x, y_augment, False, dec_target_padding_mask, padding_mask)\n",
    "        predictions = predictions[: ,i:i+1, :]\n",
    "        y_probs = tf.concat([y_probs, predictions], axis=1)\n",
    "        \n",
    "    y_probs = y_probs[:, 1:, :]\n",
    "    return tf.cast(tf.argmax(y_probs, axis=-1), tf.int32).numpy()\n",
    "\n",
    "d = data_modern[1]\n",
    "n = slice(0, 2*BATCH_SIZE)\n",
    "output = predict(d.normalized[n])\n",
    "print(list(d.normalized[n][0]))\n",
    "print(list(output[0]))\n",
    "print(list(d.niqqud[0]))\n",
    "# print(probs[0])\n",
    "(d.niqqud[n] == output).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': <tf.Tensor: shape=(), dtype=float32, numpy=0.43780378>,\n",
       " 'accuracy': <tf.Tensor: shape=(), dtype=float32, numpy=0.86848485>}"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('./checkpoints/modern')\n",
    "\n",
    "d = data_modern[1]\n",
    "n = slice(0, BATCH_SIZE)\n",
    "\n",
    "y = d.niqqud[n]\n",
    "x = d.normalized[n]\n",
    "model.test_step(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 50)\n",
      "(32, 50)\n",
      "WARNING:tensorflow:Layer decoder_48 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "[17 29 22  5 21 30  5 21 31 34 37 41 26 30  5 21 31 31 42 26 26 30 13  5\n",
      " 20 20 40 26 33 20  5 21 41 17 21  5 17 26 27  5 29 21 19 20 26 41  5  0\n",
      "  0  0]\n",
      "[ 7  1 14  1  7  1  1 15  6  2 10  6  1  1  1 15 15  2  6  6  1  1  1  1\n",
      "  2  2  6  1  2  1  1 10  2  8  1  1  7  1  2  1  2  6  2  6  1  1  1  0\n",
      "  0  0]\n",
      "[ 7  1 14  1  7  1  1 15  6  2 10  6  1  1  1 15 15 10  6  6  1  1  1  1\n",
      "  8  2  6  1  2  1  1  8  2 10  1  1  7  1  2  1  2 15  2  6  1  1  1  0\n",
      "  0  0]\n",
      "[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      " False  True  True  True  True  True  True False  True False  True  True\n",
      "  True  True  True  True  True False  True  True  True  True  True  True\n",
      "  True  True]\n",
      "0.91\n",
      "אֵלוּ הֵם הַמִסְפָרִים הַמַמְשִיִים. דְדְקִינְד הָרְאֶה אֵיךְ לְהִגְדִיר \n",
      "אֵלוּ הֵם הַמִסְפָרִים הַמַמָשִיִים. דֶדְקִינְד הֶרְאָה אֵיךְ לְהַגְדִיר \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def merge(normalized, prediction):\n",
    "    sentence = []\n",
    "    for c, n in zip(normalized, prediction):\n",
    "        if c == dataset.letters_table.PAD_TOKEN:\n",
    "            break\n",
    "        sentence.append(dataset.letters_table.indices_char[c])\n",
    "        sentence.append(dataset.niqqud_table.indices_char[n])\n",
    "    return ''.join(sentence)\n",
    "\n",
    "d = data_modern\n",
    "text = d[1].normalized[0*BATCH_SIZE:1*BATCH_SIZE]\n",
    "actual = d[1].niqqud[0*BATCH_SIZE:1*BATCH_SIZE]\n",
    "padded_actual = np.hstack([np.ones((BATCH_SIZE, 1)), actual])[:, :-1]\n",
    "print(padded_actual.shape)\n",
    "print(text.shape)\n",
    "dec_target_padding_mask = transformer.create_padding_mask(actual)\n",
    "padding_mask = transformer.create_padding_mask(text)\n",
    "prediction = model(text, padded_actual, False, dec_target_padding_mask, padding_mask)[0]  # np.argmax(history['predictions'], axis=-1)[0]\n",
    "prediction = np.argmax(prediction, axis=-1)\n",
    "n = 3\n",
    "print(text[n])\n",
    "print(prediction[n])\n",
    "print(actual[n])\n",
    "print(prediction[n] == actual[n])\n",
    "print(np.mean(prediction == actual))\n",
    "print(merge(text[n], prediction[n]))\n",
    "print(merge(text[n], actual[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.75247506, 0.97689561, 0.34302931],\n",
       "       [1.        , 0.83199473, 0.08874784, 0.22675684],\n",
       "       [1.        , 0.37551285, 0.62234334, 0.75537264],\n",
       "       [1.        , 0.97912614, 0.84942646, 0.32530607]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.random((4,3))\n",
    "x\n",
    "np.hstack([np.ones((4, 1)), x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=int32, numpy=\n",
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])>"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x = tf.concat([\n",
    "    tf.zeros((5, 1, 1), dtype=tf.float32),\n",
    "    tf.ones((5, 1, 1), dtype=tf.float32),\n",
    "    tf.zeros((5, 1, 16 - 2), dtype=tf.float32)\n",
    "], axis=-1)\n",
    "\n",
    "tf.cast(tf.argmax(x, axis=-1), tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
